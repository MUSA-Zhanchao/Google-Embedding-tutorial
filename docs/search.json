[
  {
    "objectID": "satellite-embedding/regression.html",
    "href": "satellite-embedding/regression.html",
    "title": "Visulization",
    "section": "",
    "text": "Code\nimport geemap\nimport ee\nCode\nee.Authenticate()\nCode\nee.Initialize(project=\"ee-zhanchaoyang\")\nCode\nm = geemap.Map(center=(40, -100), zoom=4)\nm\nCode\ngeometry = ee.Geometry.Polygon(\n    [[74.322, 14.981], [74.322, 14.765], [74.648, 14.765], [74.648, 14.980]]\n)\nCode\nm.add_basemap(\"SATELLITE\")\nCode\nm.addLayer(geometry, {}, \"geometry\")\nm.centerObject(geometry)\nCode\nstartDate = ee.Date.fromYMD(2022, 1, 1)\nCode\nendDate = ee.Date.fromYMD(2023, 1, 1)\nCode\nembeddings = ee.ImageCollection(\"GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL\")\nCode\nembeddingsFiltered = embeddings.filter(ee.Filter.date(startDate, endDate)).filter(\n    ee.Filter.bounds(geometry)\n)\nCode\nembeddings_vis = embeddingsFiltered.median().clip(geometry)\nCode\nvis_params = {\n    \"bands\": [\"A00\", \"A01\", \"A02\"],\n    \"min\": -0.5,\n    \"max\": 0.5,\n}\nCode\nm.addLayer(embeddings_vis, vis_params, \"embeddings\")\nm"
  },
  {
    "objectID": "satellite-embedding/regression.html#tasks-continue",
    "href": "satellite-embedding/regression.html#tasks-continue",
    "title": "Visulization",
    "section": "Tasks continue",
    "text": "Tasks continue\n\n\nCode\n# check the projections\nembeddingsProjection = ee.Image(embeddingsFiltered.first()).select(0).projection()\n\n\n\n\nCode\nembeddingsProjection\n\n\n\n\nCode\nembeddingsImage = embeddingsFiltered.mosaic().setDefaultProjection(embeddingsProjection)\n\n\n\n\nCode\ngedi = ee.ImageCollection(\"LARSE/GEDI/GEDI04_A_002_MONTHLY\")\n\n\n\n\nCode\ndef qualityMask(image):\n    mask = image.updateMask(image.select(\"l4_quality_flag\").eq(1)).updateMask(\n        image.select(\"degrade_flag\").eq(0)\n    )\n    return mask\n\n\n\n\nCode\ndef errorMask(image):\n    relative_se = image.select(\"agbd_se\").divide(image.select(\"agbd\"))\n    error_mask = image.updateMask(relative_se.lte(0.5))\n    return error_mask\n\n\n\n\nCode\ndef SlopeMask(image):\n    glo30 = ee.ImageCollection(\"COPERNICUS/DEM/GLO30\")\n    glo30Filtered = glo30.filter(ee.Filter.bounds(geometry)).select(\"DEM\")\n    demProj = glo30Filtered.first().select(0).projection()\n    elevation = glo30Filtered.mosaic().rename(\"dem\").setDefaultProjection(demProj)\n    slope = ee.Terrain.slope(elevation)\n    slope_mask = image.updateMask(slope.lte(30))\n    return slope_mask\n\n\n\n\nCode\ngediFiltered = gedi.filter(ee.Filter.date(startDate, endDate)).filter(\n    ee.Filter.bounds(geometry)\n)\n\n\n\n\nCode\ngediProjection = ee.Image(gediFiltered.first()).select(\"agbd\").projection()\n\n\n\n\nCode\ngedi_processed = gediFiltered.map(qualityMask).map(errorMask).map(SlopeMask)\n\n\n\n\nCode\ngediMosaic = gedi_processed.mosaic().select(\"agbd\").setDefaultProjection(gediProjection)\n\n\n\n\nCode\ngediVis = {\n    min: 0,\n    max: 200,\n    \"palette\": [\"#edf8fb\", \"#b2e2e2\", \"#66c2a4\", \"#2ca25f\", \"#006d2c\"],\n    \"bands\": [\"agbd\"],\n}\n\n\n\n\nCode\nm.addLayer(gedi_processed, gediVis, \"gedi\")\nm\n\n\n\n\nCode\ngridScale = 100\n\n\n\n\nCode\ngridProjection = ee.Projection(\"EPSG:3857\").atScale(gridScale)\n\n\n\n\nCode\nstacked = embeddingsImage.addBands(gediMosaic)\n\n\n\n\nCode\nstacked = stacked.resample(\"bilinear\")\n\n\n\n\nCode\nstackedResampled = stacked.reduceResolution(\n    reducer=ee.Reducer.mean(), maxPixels=1024\n).reproject(crs=gridProjection)\n\n\n\n\nCode\nstackedResampled = stackedResampled.updateMask(stackedResampled.mask().gt(0))\n\n\n\n\nCode\nexport_image = stackedResampled.clip(geometry)\n\n\n\n\nCode\ngeemap.ee_export_image_to_drive(\n    export_image,\n    description=\"GEDI_Mosaic_Export\",\n    folder=\"EarthEngine\",  # Change to your Google Drive folder\n    fileNamePrefix=\"gedi_mosaic\",  # No \".tif\" extension\n    scale=gridScale,\n    region=geometry,\n    maxPixels=1e10,\n)\n\n\n\n\nCode\npredictors = embeddingsImage.bandNames()\n\n\n\n\nCode\npredicted = gediMosaic.bandNames().get(0)\n\n\n\n\nCode\nprint(\"predictors\", predictors.getInfo())\n\n\n\n\nCode\nprint(\"predicted\", predicted.getInfo())\n\n\n\n\nCode\npredictorImage = stackedResampled.select(predictors)\n\n\n\n\nCode\npredictedImage = stackedResampled.select([predicted])\n\n\n\n\nCode\nclassMask = predictedImage.mask().toInt().rename(\"class\")\n\n\n\n\nCode\nnumSamples = 1000\n\n\n\n\nCode\ntraining = stackedResampled.addBands(classMask).stratifiedSample(\n    numPoints=numSamples,\n    classBand=\"class\",\n    region=geometry,\n    scale=gridScale,\n    classValues=[0, 1],\n    classPoints=[0, numSamples],\n    dropNulls=True,\n    tileScale=16,\n)\n\n\n\n\nCode\nmodel = (\n    ee.Classifier.smileRandomForest(50)\n    .setOutputMode(\"REGRESSION\")\n    .train(features=training, classProperty=predicted, inputProperties=predictors)\n)\n\n\n\n\nCode\npredicted = training.classify(classifier=model, outputName=\"agbd_predicted\")\n\n\n\n\nCode\ndef calculatermse(inputs):\n    observed = ee.Array(inputs.aggregate_array(\"agbd\"))\n    predicted = ee.Array(inputs.aggregate_array(\"agbd_predicted\"))\n    rmse = (\n        observed.subtract(predicted)\n        .pow(2)\n        .reduce(ee.Reducer.mean(), [0])\n        .sqrt()\n        .get([0])\n    )\n    return rmse\n\n\n\n\nCode\nrmse = calculatermse(predicted)\nprint(\"RMSE\", rmse.getInfo())"
  },
  {
    "objectID": "satellite-embedding/regression.html#plot",
    "href": "satellite-embedding/regression.html#plot",
    "title": "Visulization",
    "section": "Plot",
    "text": "Plot\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "satellite-embedding/google_earth_embedding.html",
    "href": "satellite-embedding/google_earth_embedding.html",
    "title": "Google Embeddings tutorial",
    "section": "",
    "text": "Code\nimport ee\n\n\n\n\nCode\nimport leafmap.maplibregl as leafmap\n\n\n\n\nCode\nee.Authenticate()\n\n\n\n\nCode\nee.Initialize(project=\"ee-zhanchaoyang\")\n\n\n\n\nCode\nm = leafmap.Map(projection=\"globe\", sidebar_visible=True)\nm.add_basemap(\"USGS.Imagery\")\nm.add_alphaearth_gui()\nm\n\n\n\n\nCode\nm = leafmap.Map(projection=\"globe\", sidebar_visible=True)\nm.add_basemap(\"USGS.Imagery\")\nm\n\n\n\n\nCode\nlon = -75.191\nlat = 39.9515\nm.set_center(lon, lat, zoom=16)\n\n\n\n\nCode\npoint = ee.Geometry.Point(lon, lat)\ndataset = ee.ImageCollection(\"GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL\")\n\n\n\n\nCode\nimage1 = dataset.filterDate(\"2017-01-01\", \"2018-01-01\").filterBounds(point).first()\nimage2 = dataset.filterDate(\"2024-01-01\", \"2025-01-01\").filterBounds(point).first()\n\n\n\n\nCode\nvis_params = {\"min\": -0.3, \"max\": 0.3, \"bands\": [\"A01\", \"A16\", \"A09\"]}\nm.add_ee_layer(image1, vis_params, name=\"Year 1 embeddings\")\nm.add_ee_layer(image2, vis_params, name=\"Year 2 embeddings\")\n\n\n\n\nCode\ndot_prod = image1.multiply(image2).reduce(ee.Reducer.sum())\n\n\n\n\nCode\nvis_params = {\"min\": 0, \"max\": 1, \"palette\": [\"white\", \"black\"]}\nm.add_ee_layer(dot_prod, vis_params, name=\"Similarity\")\nm"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Google Embeddings tutorial",
    "section": "",
    "text": "Author: Zhanchao Yang"
  },
  {
    "objectID": "index.html#gis-day-lightning-talk-for-the-university-of-pennsylvania",
    "href": "index.html#gis-day-lightning-talk-for-the-university-of-pennsylvania",
    "title": "Google Embeddings tutorial",
    "section": "GIS day lightning talk for the University of Pennsylvania",
    "text": "GIS day lightning talk for the University of Pennsylvania\n11.19.2025\n\nTopic\nSatellite and Population Embeddings: Powerful GeoAI Tools or Overrated? ### Abstract\nIn recent years, Google has introduced two powerful embedding for spatial analysis: satellite imagery embeddings, which capture visual and environmental patterns from overhead imagery, and population embeddings, which encode demographic and mobility information at high spatial resolutions. These embeddings promise to make geospatial machine learning more scalable, flexible, and transferable, but do they deliver? This talk will begin with an introduction to both embedding types, representation, and the types of spatial signals they capture. A short live demo will walk through two practical use cases: land use classification and estimating housing prices using these embeddings as inputs. These examples will illustrate the potential of embedding-based GeoAI to outperform traditional feature engineering. However, the talk will also highlight critical limitations: the opacity of learned representations, challenges in interpretability and validation, and the risk of reinforcing spatial and social biases. By the end, attendees will gain a clear understanding of both the capabilities and caveats of these embeddings, and when (or whether) they should be trusted in decision-making contexts."
  },
  {
    "objectID": "index.html#type-of-embedding",
    "href": "index.html#type-of-embedding",
    "title": "Google Embeddings tutorial",
    "section": "Type of Embedding",
    "text": "Type of Embedding\n\nPDFM Embedding\nGoogle Earth Embedding\n\nCredit: - Dr. Qiusheng Wu PDFM tutorial - Dr. Qiusheng Wu Google Earth Embedding tutorial - Google official JavaScript Satellite Embedding tutorial - Google PDFM Embedding - Google Earth Satellite Embedding\nALL Exert or modify from original Repo, all rights reserved to the original author!"
  },
  {
    "objectID": "index.html#population-dynamics-foundation-model-pdfm-embeddings",
    "href": "index.html#population-dynamics-foundation-model-pdfm-embeddings",
    "title": "Google Embeddings tutorial",
    "section": "Population Dynamics Foundation Model (PDFM) Embeddings",
    "text": "Population Dynamics Foundation Model (PDFM) Embeddings\nPDFM Embeddings are condensed vector representations designed to encapsulate the complex, multidimensional interactions among human behaviors, environmental factors, and local contexts at specific locations. These embeddings capture patterns in aggregated data such as search trends, busyness trends, and environmental conditions (maps, air quality, temperature), providing a rich, location-specific snapshot of how populations engage with their surroundings. Aggregated over space and time, these embeddings ensure privacy while enabling nuanced spatial analysis and prediction for applications ranging from public health to socioeconomic modeling."
  },
  {
    "objectID": "index.html#application",
    "href": "index.html#application",
    "title": "Google Embeddings tutorial",
    "section": "Application",
    "text": "Application\nPDFM Embeddings can be applied to a wide range of geospatial prediction tasks, similar to census and socioeconomic statistics. Example use cases include:\n\nPopulation Health Outcomes: Predicting health statistics like disease prevalence or population health risks.\nSocioeconomic Factors: Modeling economic indicators and living conditions.\nRetail: Identifying promising locations for new stores, market expansion, and demand forecasting.\nMarketing and Sales: Characterizing high-performance regions and identifying similar areas to optimize marketing and sales efforts.\n\nBy incorporating spatial relationships and diverse feature types, these embeddings serve as a powerful tool for geospatial predictions."
  },
  {
    "objectID": "pdfm/zillow_pdfm.html",
    "href": "pdfm/zillow_pdfm.html",
    "title": "Google Embeddings tutorial",
    "section": "",
    "text": "Code\nimport os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom leafmap.common import evaluate_model, plot_actual_vs_predicted, download_file\n\n\n\n\nCode\nzhvi_file = \"/home/zyang91/Desktop/data/zillow_home_value_index_by_zipcode.csv\"\n\n\n\n\nCode\nzhvi_df = pd.read_csv(zhvi_file, dtype={\"RegionName\": str})\nzhvi_df.index = zhvi_df[\"RegionName\"].apply(lambda x: f\"zip/{x}\")\nzhvi_df\n\n\n\n\nCode\nembeddings_file = \"/home/zyang91/Desktop/us/zcta_embeddings.csv\"\n\n\n\n\nCode\nzipcode_embeddings = pd.read_csv(embeddings_file).set_index(\"place\")\nzipcode_embeddings\n\n\n\n\nCode\ndata = zhvi_df.join(zipcode_embeddings, how=\"inner\")\ndata\n\n\n\n\nCode\nembedding_features = [f\"feature{x}\" for x in range(330)]\nlabel = \"2024-10-31\"\n\n\n\n\nCode\ndata = data.dropna(subset=[label])\ndata\n\n\n\n\nCode\ndata = data[embedding_features + [label]]\nX = data[embedding_features]\ny = data[label]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n\n\n\nCode\n# Initialize and train a simple linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred = model.predict(X_test)\n\n\n\n\nCode\nevaluation_df = pd.DataFrame({\"y\": y_test, \"y_pred\": y_pred})\nmetrics = evaluate_model(evaluation_df)\nprint(metrics)\n\n\n\n\nCode\nxy_lim = (0, 3_000_000)\nplot_actual_vs_predicted(\n    evaluation_df,\n    xlim=xy_lim,\n    ylim=xy_lim,\n    title=\"Actual vs Predicted Home Values\",\n    x_label=\"Actual Home Value\",\n    y_label=\"Predicted Home Value\",\n)\n\n\n\n\nCode\nevaluate_df = pd.DataFrame({\"y\": y_test, \"y_pred\": y_pred})\nmetrics = evaluate_model(evaluate_df)\nprint(metrics)\n\n\n\n\nCode\nevaluate_df.head()\n\n\n\n\nCode\nxy_lim = (0, 3000000)\nplot_actual_vs_predicted(\n    evaluate_df,\n    x_label=\"Actual\",\n    y_label=\"Predicted\",\n    xlim=xy_lim,\n    ylim=xy_lim,\n    title=\"Actual vs Predicted ZHVI\",\n)\n\n\n\n\nCode\nk = 5\nknn_model = KNeighborsRegressor(n_neighbors=k)\nknn_model.fit(X_train, y_train)\ny_pred = knn_model.predict(X_test)\n\n\n\n\nCode\nplot_actual_vs_predicted(\n    evaluate_df,\n    x_label=\"Actual\",\n    y_label=\"Predicted\",\n    xlim=xy_lim,\n    ylim=xy_lim,\n    title=f\"Actual vs Predicted ZHVI (KNN, k={k})\",\n)\n\n\n\n\nCode\nevaluation_df = pd.DataFrame({\"y\": y_test, \"y_pred\": y_pred})\nmetrics = evaluate_model(evaluation_df)\nprint(metrics)"
  },
  {
    "objectID": "pdfm/zillow_pdfm_visu.html",
    "href": "pdfm/zillow_pdfm_visu.html",
    "title": "Google Embeddings tutorial",
    "section": "",
    "text": "Code\nimport os\nimport pandas as pd\nimport geopandas as gpd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom leafmap.common import evaluate_model, plot_actual_vs_predicted, download_file\nimport leafmap.maplibregl as leafmap\n\n\n\n\nCode\nzhvi = \"https://github.com/opengeos/datasets/releases/download/us/zillow_home_value_index_by_county.csv\"\nzhvo_file = \"zillow_home_value_index_by_county.csv\"\nif not os.path.exists(zhvo_file):\n    download_file(zhvi, zhvo_file)\n\n\n\n\nCode\nzhvi_df = pd.read_csv(zhvo_file, dtype={\"StateCodeFIPS\": str, \"MunicipalCodeFIPS\": str})\nzhvi_df.index = \"geoId/\" + zhvi_df[\"StateCodeFIPS\"] + zhvi_df[\"MunicipalCodeFIPS\"]\nzhvi_df.head()\n\n\n\n\nCode\ncounty_geojson = \"/home/zyang91/Desktop/us/county.geojson\"\n\n\n\n\nCode\ncounty_gdf = gpd.read_file(county_geojson)\ncounty_gdf.set_index(\"place\", inplace=True)\ncounty_gdf.head()\n\n\n\n\nCode\ndf = zhvi_df.join(county_gdf)\ndf\n\n\n\n\nCode\nzhvi_gdf = gpd.GeoDataFrame(df, geometry=\"geometry\")\nzhvi_gdf.head()\n\n\n\n\nCode\ncolumn = \"2024-10-31\"\ngdf = zhvi_gdf[[\"RegionName\", \"State\", column, \"geometry\"]]\ngdf.head()\n\n\n\n\nCode\nm = leafmap.Map(style=\"liberty\")\nm.add_data(\n    gdf,\n    cmap=\"Blues\",\n    column=column,\n    legend_title=\"Zillow Home Median Home Value\",\n    name=\"Zillow Home Median Home Value\",\n)\nm.add_layer_control()\nm\n\n\n\n\nCode\nm = leafmap.Map(style=\"liberty\", pitch=60)\nm.add_data(\n    gdf,\n    cmap=\"Blues\",\n    column=column,\n    extrude=True,\n    scale_factor=3,\n    legend_title=\"Zillow Home Median Home Value\",\n    name=\"Zillow Home Median Home Value\",\n)\nm.add_layer_control()\nm\n\n\n\n\nCode\nembeddings = pd.read_csv(\"/home/zyang91/Desktop/us/county_embeddings.csv\").set_index(\n    \"place\"\n)\nembeddings.head()\n\n\n\n\nCode\ndf = embeddings.join(county_gdf)\n\n\n\n\nCode\nembeddings_gdf = gpd.GeoDataFrame(df, geometry=\"geometry\")\nembeddings_gdf.head()\n\n\n\n\nCode\ncolumn = \"feature329\"\ngdf = embeddings_gdf[[\"state\", column, \"geometry\"]]\ngdf.head()\n\n\n\n\nCode\nm = leafmap.Map(style=\"liberty\")\nm.add_data(\n    gdf,\n    cmap=\"Blues\",\n    column=column,\n    legend_title=column,\n    name=column,\n)\nm.add_layer_control()\nm\n\n\n\n\nCode\nm = leafmap.Map(style=\"liberty\", pitch=60)\nm.add_data(\n    gdf,\n    cmap=\"Blues\",\n    column=column,\n    extrude=True,\n    scale_factor=0.00005,\n    legend_title=column,\n    name=column,\n)\nm.add_layer_control()\nm\n\n\n\n\nCode\ndata = zhvi_df.join(embeddings, how=\"inner\")\ndata.head()\n\n\n\n\nCode\nembedding_features = [f\"feature{i}\" for i in range(330)]\nlabel = \"2024-10-31\"\n\n\n\n\nCode\ndata = data.dropna(subset=[label])\n\n\n\n\nCode\ndata = data[embedding_features + [label]]\nx = data[embedding_features]\ny = data[label]\nx_train, x_test, y_train, y_test = train_test_split(\n    x, y, test_size=0.2, random_state=42\n)\n\n\n\n\nCode\nmodel = LinearRegression()\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\n\n\n\n\nCode\nevaluation_df = pd.DataFrame({\"y\": y_test, \"y_pred\": y_pred})\nmetrics = evaluate_model(evaluation_df)\nprint(metrics)\n\n\n\n\nCode\nxy_lim = (0, 1000000)\nplot_actual_vs_predicted(\n    evaluation_df,\n    xlim=xy_lim,\n    ylim=xy_lim,\n    title=\"Linear Regression: Actual vs Predicted\",\n    x_label=\"Actual Home Value\",\n    y_label=\"Predicted Home Value\",\n)\n\n\n\n\nCode\ndf = evaluation_df.join(gdf)\ndf[\"difference\"] = df[\"y\"] - df[\"y_pred\"]\n\n\n\n\nCode\ndf.head()\n\n\n\n\nCode\ngdf = gpd.GeoDataFrame(df, geometry=\"geometry\")\ngdf.head()\n\n\n\n\nCode\ngdf.drop(columns=[\"category\", \"color\", column], inplace=True)\ngdf.head()\n\n\n\n\nCode\nm = leafmap.Map(style=\"liberty\")\nm.add_data(\n    gdf,\n    cmap=\"Blues\",\n    column=\"difference\",\n    legend_title=\"Difference (Actual - Predicted)\",\n    name=\"Difference\",\n)\nm.add_layer_control()\nm"
  },
  {
    "objectID": "slides/slides.html#about-me",
    "href": "slides/slides.html#about-me",
    "title": "Satellite and Population Embedding",
    "section": "About Me",
    "text": "About Me\n\n\n\n\n\n\nZhanchao Yang\n\nMaster of City Planning (MCP)\nMaster of Urban Spatial Analytics (MUSA)\nBachelor of Arts in Geography, SUNY Binghamton\nResearch Assistant, Weitzman School of Design"
  },
  {
    "objectID": "slides/slides.html#what-is-embeddings",
    "href": "slides/slides.html#what-is-embeddings",
    "title": "Satellite and Population Embedding",
    "section": "What is Embeddings?",
    "text": "What is Embeddings?\n\nIn general terms, embeddings are numerical representations of complex information that capture the most important features or relationships in a simplified, machine-readable form."
  },
  {
    "objectID": "slides/slides.html#blackbox-nature-of-embeddings",
    "href": "slides/slides.html#blackbox-nature-of-embeddings",
    "title": "Satellite and Population Embedding",
    "section": "Blackbox Nature of Embeddings",
    "text": "Blackbox Nature of Embeddings\n\n\n\nGenerated through various neural network models (GNN or CNN) trained on a large datasets.\nExact architecture and training process vary widely.\nThe company or research institutions may not disclose the full details of their models."
  },
  {
    "objectID": "slides/slides.html#population-dynamics-foundation-pdfm",
    "href": "slides/slides.html#population-dynamics-foundation-pdfm",
    "title": "Satellite and Population Embedding",
    "section": "Population Dynamics Foundation (PDFM)",
    "text": "Population Dynamics Foundation (PDFM)\n\n\n\nDeveloped by Google Research (Google Deep Mind) in 2024.\nInput:\n\nCensus data\nSearching Trends\nOther geospatial data\n\nOutput:\n\nMulti-dimensional vectors (represent population characteristics and dynamics)."
  },
  {
    "objectID": "slides/slides.html#how-pdfm-embeddings-look-like",
    "href": "slides/slides.html#how-pdfm-embeddings-look-like",
    "title": "Satellite and Population Embedding",
    "section": "How PDFM Embeddings look like?",
    "text": "How PDFM Embeddings look like?\n\n\n\n330-dimensional vectors for each census tract or ZIP code\nEach dimension captures different aspects of population dynamics\nExamples of captured features:\n\nMobility patterns\nSearch behavior trends\nLocal economic activity\nEnvironmental conditions"
  },
  {
    "objectID": "slides/slides.html#applications-of-pdfm-embeddings",
    "href": "slides/slides.html#applications-of-pdfm-embeddings",
    "title": "Satellite and Population Embedding",
    "section": "Applications of PDFM Embeddings",
    "text": "Applications of PDFM Embeddings\n\n\nHealth & Social Services:\n\nDisease prevalence prediction\nHealthcare resource allocation\n\nEconomic Analysis:\n\nSocioeconomic indicators\nIncome and poverty estimation\n\nUrban Planning:\n\nPopulation growth forecasting\nPOI and hotspot prediction"
  },
  {
    "objectID": "slides/slides.html#additional-applications",
    "href": "slides/slides.html#additional-applications",
    "title": "Satellite and Population Embedding",
    "section": "Additional Applications",
    "text": "Additional Applications"
  },
  {
    "objectID": "slides/slides.html#demo-1-using-pdfm-embeddings-to-predict-housing-prices",
    "href": "slides/slides.html#demo-1-using-pdfm-embeddings-to-predict-housing-prices",
    "title": "Satellite and Population Embedding",
    "section": "Demo 1: Using PDFM Embeddings to predict housing prices",
    "text": "Demo 1: Using PDFM Embeddings to predict housing prices\nDataset:\n\nZillow Home Value Index (ZHVI) by ZIP code\nPDFM embeddings (326 features)\n\nModel:\n\nLinear Regression (Stepwise regression)\nData mining approaches\nOutput: Predicted home prices"
  },
  {
    "objectID": "slides/slides.html#satellite-foundation-model-sfm-embeddings",
    "href": "slides/slides.html#satellite-foundation-model-sfm-embeddings",
    "title": "Satellite and Population Embedding",
    "section": "Satellite Foundation Model (SFM) Embeddings",
    "text": "Satellite Foundation Model (SFM) Embeddings\n\nDeveloped by Google Research\nBased on Sentinel-2 satellite imagery\nGlobal coverage (2017-2023)\nSpatial resolution: 10 meters\nAvailable through Google Earth Engine"
  },
  {
    "objectID": "slides/slides.html#what-satellite-embeddings-capture",
    "href": "slides/slides.html#what-satellite-embeddings-capture",
    "title": "Satellite and Population Embedding",
    "section": "What Satellite Embeddings Capture?",
    "text": "What Satellite Embeddings Capture?\n\nLand cover patterns\nVegetation characteristics\nUrban development\nEnvironmental changes\nTemporal dynamics\n\nTrained on massive satellite imagery corpus using self-supervised learning"
  },
  {
    "objectID": "slides/slides.html#satellite-embeddings-with-google-earth-engine-gee",
    "href": "slides/slides.html#satellite-embeddings-with-google-earth-engine-gee",
    "title": "Satellite and Population Embedding",
    "section": "Satellite Embeddings with Google Earth Engine (GEE)",
    "text": "Satellite Embeddings with Google Earth Engine (GEE)\nDataset:\n\nGOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL\n70 bands\n\nKey Operations:\n\nFilter by date and location\nExtract embeddings for points/regions\nCompare temporal changes\nCalculate similarity (dot product)"
  },
  {
    "objectID": "slides/slides.html#applications-of-sfm-embeddings",
    "href": "slides/slides.html#applications-of-sfm-embeddings",
    "title": "Satellite and Population Embedding",
    "section": "Applications of SFM Embeddings",
    "text": "Applications of SFM Embeddings\nEnvironmental and Land Use Planning\n\nUrban sprawl analysis\nLand cover classification\nChange detection\nDevelopment planning\n\nTemporal Analysis:\n\nSeasonal pattern detection\nLong-term trend analysis\nBefore/after comparisons\nSimilarity searches across time"
  },
  {
    "objectID": "slides/slides.html#demo-2-satellite-embeddings-for-land-cover-classification",
    "href": "slides/slides.html#demo-2-satellite-embeddings-for-land-cover-classification",
    "title": "Satellite and Population Embedding",
    "section": "Demo 2: Satellite Embeddings for Land Cover Classification",
    "text": "Demo 2: Satellite Embeddings for Land Cover Classification\nApproach:\n\nUse embeddings as features\nTrain classification model\nPredict land cover types\n\nAdvantages:\n\nNo need to process raw imagery\nPre-trained representations\nFaster than traditional methods"
  },
  {
    "objectID": "slides/slides.html#workflow",
    "href": "slides/slides.html#workflow",
    "title": "Satellite and Population Embedding",
    "section": "Workflow",
    "text": "Workflow\n\nDefine area of interest\nLoad embedding ImageCollection\nExtract features for labeled samples\nTrain classifier (Random Forest, SVM)\nApply to entire region\nValidate results\n\nUses Earth Engine’s cloud computing for scalability"
  },
  {
    "objectID": "slides/slides.html#limitations-and-concerns",
    "href": "slides/slides.html#limitations-and-concerns",
    "title": "Satellite and Population Embedding",
    "section": "Limitations and Concerns",
    "text": "Limitations and Concerns\nInterpretability Issues:\n\nBlack box representations\nLimited transparency in training process\n\nBias and Fairness:\n\nMay encode historical biases\nRisk of perpetuating spatial inequalities\n\nTechnical Limitations:\n\nResolution constraints (10m for satellite)\nUpdates not in real-time\n\n…"
  },
  {
    "objectID": "slides/slides.html#summary-and-conclusions",
    "href": "slides/slides.html#summary-and-conclusions",
    "title": "Satellite and Population Embedding",
    "section": "Summary and Conclusions",
    "text": "Summary and Conclusions\nThe Promise:\n\nPowerful tools for spatial analysis\nCapture complex patterns (Privacy concerns)\n\nThe Reality:\n\nRequire careful validation\nComplement, not replace\n\nBest Practices:\n\nValidate predictions thoroughly\nBe aware of limitations and biases"
  },
  {
    "objectID": "slides/slides.html#final-verdict",
    "href": "slides/slides.html#final-verdict",
    "title": "Satellite and Population Embedding",
    "section": "Final Verdict",
    "text": "Final Verdict\nPowerful GeoAI tools, but use with caution and critical thinking!"
  },
  {
    "objectID": "slides/slides.html#references",
    "href": "slides/slides.html#references",
    "title": "Satellite and Population Embedding",
    "section": "References",
    "text": "References\n\nGoogle Research:\n\nGoogle PDFM Embeddings: github.com/google-research/population-dynamics\nSatellite Embeddings Dataset: developers.google.com/earth-engine/datasets/catalog/GOOGLE_SATELLITE_EMBEDDING_V1_ANNUAL\n\nTutorials & Resources:\n\nDr. Qiusheng Wu PDFM Tutorial: github.com/opengeos/GeoAI-Tutorials\nDr. Qiusheng Wu Earth Embedding Tutorial: leafmap.org/maplibre/AlphaEarth\nGoogle Earth Engine Tutorial: developers.google.com/earth-engine/tutorials/community/satellite-embedding-02-unsupervised-classification\n\nThis Presentation:\n\nGitHub Repository: github.com/zyang91/Google-Embedding-tutorial"
  },
  {
    "objectID": "satellite-embedding/demo2/landcover-classification.html",
    "href": "satellite-embedding/demo2/landcover-classification.html",
    "title": "Unsupervised Classification with Satellite Embedding Dataset",
    "section": "",
    "text": "Author: Zhanchao Yang  Weitzman School of Design, University of Pennsylvania"
  },
  {
    "objectID": "satellite-embedding/demo2/landcover-classification.html#acknowledgements",
    "href": "satellite-embedding/demo2/landcover-classification.html#acknowledgements",
    "title": "Unsupervised Classification with Satellite Embedding Dataset",
    "section": "1 Acknowledgements",
    "text": "1 Acknowledgements\nThis tutorial is adapted from the official Google Earth Engine embedding tutorial: https://developers.google.com/earth-engine/tutorials/community/satellite-embedding-02-unsupervised-classification"
  },
  {
    "objectID": "satellite-embedding/demo2/landcover-classification.html#overview",
    "href": "satellite-embedding/demo2/landcover-classification.html#overview",
    "title": "Unsupervised Classification with Satellite Embedding Dataset",
    "section": "2 Overview",
    "text": "2 Overview\nIn this tutorial, we will take an unsupervised classification approach to ise the Satellite Embedding Dataset to classify land cover types in a study area. We will use the K-Means clustering algorithm to group similar land cover types based on their spectral characteristics.\n\n2.1 A glance of the Google Satellite Embedding Dataset\nWe used the leafmap package to quickly visualize random bands from the Satellite Embedding Dataset across Pennsylvania on an interactive map. See the note book on the GitHub repo for details thanks to amazing tutorial by Dr. Qiusheng Wu.\nCombination one(random 3 draw from 64 bands)\n\nCombination two(random 3 draw from 64 bands)"
  },
  {
    "objectID": "satellite-embedding/demo2/landcover-classification.html#load-libraries-and-google-earth-engine-authentication",
    "href": "satellite-embedding/demo2/landcover-classification.html#load-libraries-and-google-earth-engine-authentication",
    "title": "Unsupervised Classification with Satellite Embedding Dataset",
    "section": "3 Load Libraries and Google Earth Engine Authentication",
    "text": "3 Load Libraries and Google Earth Engine Authentication\n\n\nShow Code\nimport ee\nimport geemap\n\n\n\n\nShow Code\nee.Authenticate()\nee.Initialize(project=\"ee-zhanchaoyang\")"
  },
  {
    "objectID": "satellite-embedding/demo2/landcover-classification.html#defined-study-area",
    "href": "satellite-embedding/demo2/landcover-classification.html#defined-study-area",
    "title": "Unsupervised Classification with Satellite Embedding Dataset",
    "section": "4 Defined study area",
    "text": "4 Defined study area\nLancaster County in Pennsylvania is one of the most productive agricultural counties in the United States. It is known for its fertile soil and favorable climate, which support a wide variety of crops. The county is particularly famous for its corn and soybean production, which are the two main crops grown in the area. In addition to these staple crops, Lancaster County also produces wheat, barley, oats, and various fruits and vegetables. The county’s agricultural landscape is characterized by a mix of small family farms and larger commercial operations, contributing to its reputation as a leading agricultural region.\n\n\nShow Code\ncounties = ee.FeatureCollection(\"TIGER/2018/Counties\")\n\n\n\n\nShow Code\nlancaster = counties.filter(ee.Filter.eq(\"GEOID\", \"42071\")).geometry()\n\n\n\n\nShow Code\nm = geemap.Map(center=[40.04, -76.30], zoom=9)\nm.addLayer(lancaster, {}, \"Lancaster County\")\nm"
  },
  {
    "objectID": "satellite-embedding/demo2/landcover-classification.html#loading-satellite-embedding-and-training-dataset",
    "href": "satellite-embedding/demo2/landcover-classification.html#loading-satellite-embedding-and-training-dataset",
    "title": "Unsupervised Classification with Satellite Embedding Dataset",
    "section": "5 Loading satellite embedding and training dataset",
    "text": "5 Loading satellite embedding and training dataset\n\n\nShow Code\nembedding = ee.ImageCollection(\"GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL\")\n\n\n\n\nShow Code\nyear = 2022\nstartdate = ee.Date.fromYMD(year, 1, 1)\nenddate = ee.Date.fromYMD(year + 1, 1, 1)\n\n\n\n\nShow Code\nstudy_embeddings = embedding.filter(ee.Filter.date(startdate, enddate)).filter(\n    ee.Filter.bounds(lancaster)\n);\n\n\n\n\nShow Code\nembeddingsImage = study_embeddings.mosaic()\n\n\n\n5.1 loading training data: USDA-NASS Cropland Data Layer (CDL)\nFor our modeling, we need to exclude non-cropland areas. There are many global and regional datasets that can be used to create a crop mask. ESA WorldCover or GFSAD Global Cropland Extent Product are good choices for global cropland datasets. A more recent addition is the ESA WorldCereal Active Cropland product which has seasonal mapping of active croplands. Since our region is in the US, we can use a more accurate regional dataset USDA NASS Cropland Data Layers (CDL) to obtain a crop mask.\n\n\nShow Code\ncdl = (\n    ee.ImageCollection(\"USDA/NASS/CDL\")\n    .filter(ee.Filter.date(\"2022-01-01\", \"2023-01-01\"))\n    .first()\n)\ncropland = cdl.select(\"cropland\")\ncropland_mask = cdl.select(\"cultivated\").eq(2).rename(\"cropmask\")\n\n\n\n\nShow Code\nmap = geemap.Map(center=[40.04, -76.30], zoom=9)\nm.addLayer(\n    cropland_mask.clip(lancaster),\n    {\"min\": 0, \"max\": 1, \"palette\": [\"white\", \"green\"]},\n    \"Cropland Mask\",\n)\nm"
  },
  {
    "objectID": "satellite-embedding/demo2/landcover-classification.html#extract-training-samples",
    "href": "satellite-embedding/demo2/landcover-classification.html#extract-training-samples",
    "title": "Unsupervised Classification with Satellite Embedding Dataset",
    "section": "6 Extract training samples",
    "text": "6 Extract training samples\nWe apply the cropland mask to the embedding mosaic. We are now left with all the pixels representing cultivated cropland in the county.\n\n\nShow Code\ncluster_image = embeddingsImage.updateMask(cropland_mask).addBands(cropland_mask)\n\n\nWe need to take the Satellite Embedding image and obtain random samples to train a clustering model. Since our region of interest contains many masked pixels, a simple random sampling may result in samples with null values. To ensure we can extract the desired number of non-null samples, we use stratified sampling to obtain the desired number of samples in unmasked areas.\n\n\nShow Code\ntraining = cluster_image.stratifiedSample(\n    numPoints=1000,\n    classBand=\"cropmask\",\n    region=lancaster,\n    scale=10,\n    tileScale=16,\n    seed=100,\n    dropNulls=True,\n    geometries=True,\n)\n\n\n\n\nShow Code\nm.addLayer(training.style(**{\"color\": \"red\", \"pointSize\": 3}), {}, \"Training Points\")\nm"
  },
  {
    "objectID": "satellite-embedding/demo2/landcover-classification.html#perform-k-means-clustering",
    "href": "satellite-embedding/demo2/landcover-classification.html#perform-k-means-clustering",
    "title": "Unsupervised Classification with Satellite Embedding Dataset",
    "section": "7 Perform K-Means Clustering",
    "text": "7 Perform K-Means Clustering\nWe can now train a clusterer and group the 64D embedding vectors into a chosen number of distinct clusters. We can perform unsupervised clustering on the Satellite Embedding to obtain clusters of pixels that have similar temporal trajectories and patterns. Pixels with similar spectral and spatial characteristics along with similar phenology will be grouped in the same cluster.\nThe ee.Clusterer.wekaCascadeKMeans() allows us to specify a minimum and maximum number of clusters and find the optimal number of clusters based on the training data.\n\n\nShow Code\nmincluster = 4\nmaxcluster = 5\n\n\n\n\nShow Code\nclusterer = ee.Clusterer.wekaCascadeKMeans(\n    minClusters=mincluster,\n    maxClusters=maxcluster,\n).train(features=training, inputProperties=cluster_image.bandNames())\n\nclustered = cluster_image.cluster(clusterer)\n\n\n\n\nShow Code\nvis = clustered.randomVisualizer().clip(lancaster)\nm.addLayer(vis, {}, \"Clustered Image\")\nm\n\n\n\n\nShow Code\narea_image = ee.Image.pixelArea().divide(4046.86).addBands(clustered)\n\n\n\n\nShow Code\nareas = area_image.reduceRegion(\n    reducer=ee.Reducer.sum().group(\n        groupField=1,\n        groupName=\"cluster\",\n    ),\n    geometry=lancaster,\n    scale=10,\n    maxPixels=1e10,\n)\n\n\n\n\nShow Code\nprint(areas.getInfo())\n\n\n\n\nShow Code\ncluster_areas = ee.List(areas.get(\"groups\"))\n\n\n\n\nShow Code\nclusterAreas = ee.List(cluster_areas)\n\n\n\n\nShow Code\ndef to_feature(item):\n    d = ee.Dictionary(item)\n    return ee.Feature(\n        None, {\"cluster\": d.getNumber(\"cluster\").format(), \"area\": d.getNumber(\"sum\")}\n    )\n\n\ncluster_area_fc = ee.FeatureCollection(cluster_areas.map(to_feature))\n\n\n\n\nShow Code\nprint(cluster_area_fc.limit(10).getInfo())\n\n\nPrediction results (in acres): - Cluster 1: 25515.6423 - Cluster 2: 119071.5298 - Cluster 3: 62110.8848 - Cluster 4: 56379.1821\n\n7.1 Limitations of K-Means Clustering:\n\nWe need local knowledge to understand the optimal number of clusters to use for our analysis.\nWe also need local knowledge to figure out the land cover types represented by each cluster."
  },
  {
    "objectID": "satellite-embedding/demo2/landcover-classification.html#validating-classification-results",
    "href": "satellite-embedding/demo2/landcover-classification.html#validating-classification-results",
    "title": "Unsupervised Classification with Satellite Embedding Dataset",
    "section": "8 Validating classification results",
    "text": "8 Validating classification results\nBased on the USDA report, the main crops in Lancaster County are: - Corn for grain 95,549 + 35,988 = 131537; Prediction=119071 - Forage (hay/haylage), all 65,142 (others) - Soybeans for beans 51,695 - Wheat for grain, all 24,101\nWe try to group the crops into 3 clusters: Corn, Soybean, and Other crops for simplicity. \n\n\nShow Code\ncdl = (\n    ee.ImageCollection(\"USDA/NASS/CDL\")\n    .filter(ee.Filter.date(\"2022-01-01\", \"2023-01-01\"))\n    .first()\n)\ncropland = cdl.select(\"cropland\")\ncropmap = cropland.updateMask(cropland_mask).rename(\"crops\")\n\n\n\n\nShow Code\ncropclasses = ee.List.sequence(0, 254)\n\n\n\n\nShow Code\ntargetclasses = ee.List.repeat(0, 255).set(1, 1).set(5, 2)\n\n\n\n\nShow Code\ncropmapreclass = cropmap.remap(cropclasses, targetclasses).rename(\"crops\")\n\n\n\n\nShow Code\ncrop_vis = {\"min\": 0, \"max\": 2, \"palette\": [\"#bdbdbd\", \"#ffd400\", \"#267300\"]}\nm.addLayer(cropmapreclass.clip(lancaster), crop_vis, \"Reclassified Crop Map\")\nm\n\n\nValidation Results from USDA-NASS CDL 2022:\n\nUnsupervised classification results: from the Kmeans clustering on Satellite Embedding Dataset\n\nHighly suggest to run the code cells in the notebook to explore the interactive map to compare the classification results with the USDA-NASS CDL 2022 data."
  },
  {
    "objectID": "satellite-embedding/demo2/landcover-classification.html#references-and-acknowledgements",
    "href": "satellite-embedding/demo2/landcover-classification.html#references-and-acknowledgements",
    "title": "Unsupervised Classification with Satellite Embedding Dataset",
    "section": "9 References and acknowledgements",
    "text": "9 References and acknowledgements\n\nGoogle Earth Engine Satellite Embedding Tutorial by Ujaval Gandhi via official Google Earth Engine embedding tutorial: https://developers.google.com/earth-engine/tutorials/community/satellite-embedding-02-unsupervised-classification\nLeafmap package by Dr. Qiusheng Wu: https://leafmap.org/\ngeemap package by Dr. Qiusheng Wu: https://geemap.org/\nDr. Qiusheng Wu Google Satellite Embedding tutorial: https://www.youtube.com/watch?v=EGL7fXyA7-U\n\nThank you so much to Ujaval Gandhi and Dr. Qiusheng Wu for their amazing tutorials and open-source codes!"
  }
]