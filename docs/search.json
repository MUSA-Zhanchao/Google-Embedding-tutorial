[
  {
    "objectID": "SECURITY.html",
    "href": "SECURITY.html",
    "title": "Security Policy",
    "section": "",
    "text": "Use this section to tell people about which versions of your project are currently being supported with security updates.\n\n\n\nVersion\nSupported\n\n\n\n\n5.1.x\n:white_check_mark:\n\n\n5.0.x\n:x:\n\n\n4.0.x\n:white_check_mark:\n\n\n&lt; 4.0\n:x:\n\n\n\n\n\n\nUse this section to tell people how to report a vulnerability.\nTell them where to go, how often they can expect to get an update on a reported vulnerability, what to expect if the vulnerability is accepted or declined, etc."
  },
  {
    "objectID": "SECURITY.html#supported-versions",
    "href": "SECURITY.html#supported-versions",
    "title": "Security Policy",
    "section": "",
    "text": "Use this section to tell people about which versions of your project are currently being supported with security updates.\n\n\n\nVersion\nSupported\n\n\n\n\n5.1.x\n:white_check_mark:\n\n\n5.0.x\n:x:\n\n\n4.0.x\n:white_check_mark:\n\n\n&lt; 4.0\n:x:"
  },
  {
    "objectID": "SECURITY.html#reporting-a-vulnerability",
    "href": "SECURITY.html#reporting-a-vulnerability",
    "title": "Security Policy",
    "section": "",
    "text": "Use this section to tell people how to report a vulnerability.\nTell them where to go, how often they can expect to get an update on a reported vulnerability, what to expect if the vulnerability is accepted or declined, etc."
  },
  {
    "objectID": "satellite-embedding/demo2/landcover-classification.html",
    "href": "satellite-embedding/demo2/landcover-classification.html",
    "title": "Unsupervised Classification with Satellite Embedding Dataset",
    "section": "",
    "text": "Author: Zhanchao Yang  Weitzman School of Design, University of Pennsylvania"
  },
  {
    "objectID": "satellite-embedding/demo2/landcover-classification.html#acknowledgements",
    "href": "satellite-embedding/demo2/landcover-classification.html#acknowledgements",
    "title": "Unsupervised Classification with Satellite Embedding Dataset",
    "section": "1 Acknowledgements",
    "text": "1 Acknowledgements\nThis tutorial is adapted from the official Google Earth Engine embedding tutorial: https://developers.google.com/earth-engine/tutorials/community/satellite-embedding-02-unsupervised-classification"
  },
  {
    "objectID": "satellite-embedding/demo2/landcover-classification.html#overview",
    "href": "satellite-embedding/demo2/landcover-classification.html#overview",
    "title": "Unsupervised Classification with Satellite Embedding Dataset",
    "section": "2 Overview",
    "text": "2 Overview\nIn this tutorial, we will take an unsupervised classification approach to ise the Satellite Embedding Dataset to classify land cover types in a study area. We will use the K-Means clustering algorithm to group similar land cover types based on their spectral characteristics.\n\n2.1 A glance of the Google Satellite Embedding Dataset\nWe used the leafmap package to quickly visualize random bands from the Satellite Embedding Dataset across Pennsylvania on an interactive map. See the note book on the GitHub repo for details thanks to amazing tutorial by Dr. Qiusheng Wu.\nCombination one(random 3 draw from 64 bands)\n\nCombination two(random 3 draw from 64 bands)"
  },
  {
    "objectID": "satellite-embedding/demo2/landcover-classification.html#load-libraries-and-google-earth-engine-authentication",
    "href": "satellite-embedding/demo2/landcover-classification.html#load-libraries-and-google-earth-engine-authentication",
    "title": "Unsupervised Classification with Satellite Embedding Dataset",
    "section": "3 Load Libraries and Google Earth Engine Authentication",
    "text": "3 Load Libraries and Google Earth Engine Authentication\n\n\nShow Code\nimport ee\nimport geemap\n\n\n\n\nShow Code\nee.Authenticate()\nee.Initialize(project=\"ee-zhanchaoyang\")"
  },
  {
    "objectID": "satellite-embedding/demo2/landcover-classification.html#defined-study-area",
    "href": "satellite-embedding/demo2/landcover-classification.html#defined-study-area",
    "title": "Unsupervised Classification with Satellite Embedding Dataset",
    "section": "4 Defined study area",
    "text": "4 Defined study area\nLancaster County in Pennsylvania is one of the most productive agricultural counties in the United States. It is known for its fertile soil and favorable climate, which support a wide variety of crops. The county is particularly famous for its corn and soybean production, which are the two main crops grown in the area. In addition to these staple crops, Lancaster County also produces wheat, barley, oats, and various fruits and vegetables. The county’s agricultural landscape is characterized by a mix of small family farms and larger commercial operations, contributing to its reputation as a leading agricultural region.\n\n\nShow Code\ncounties = ee.FeatureCollection(\"TIGER/2018/Counties\")\n\n\n\n\nShow Code\nlancaster = counties.filter(ee.Filter.eq(\"GEOID\", \"42071\")).geometry()\n\n\n\n\nShow Code\nm = geemap.Map(center=[40.04, -76.30], zoom=9)\nm.addLayer(lancaster, {}, \"Lancaster County\")\nm"
  },
  {
    "objectID": "satellite-embedding/demo2/landcover-classification.html#loading-satellite-embedding-and-training-dataset",
    "href": "satellite-embedding/demo2/landcover-classification.html#loading-satellite-embedding-and-training-dataset",
    "title": "Unsupervised Classification with Satellite Embedding Dataset",
    "section": "5 Loading satellite embedding and training dataset",
    "text": "5 Loading satellite embedding and training dataset\n\n\nShow Code\nembedding = ee.ImageCollection(\"GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL\")\n\n\n\n\nShow Code\nyear = 2022\nstartdate = ee.Date.fromYMD(year, 1, 1)\nenddate = ee.Date.fromYMD(year + 1, 1, 1)\n\n\n\n\nShow Code\nstudy_embeddings = embedding.filter(ee.Filter.date(startdate, enddate)).filter(\n    ee.Filter.bounds(lancaster)\n);\n\n\n\n\nShow Code\nembeddingsImage = study_embeddings.mosaic()\n\n\n\n5.1 loading training data: USDA-NASS Cropland Data Layer (CDL)\nFor our modeling, we need to exclude non-cropland areas. There are many global and regional datasets that can be used to create a crop mask. ESA WorldCover or GFSAD Global Cropland Extent Product are good choices for global cropland datasets. A more recent addition is the ESA WorldCereal Active Cropland product which has seasonal mapping of active croplands. Since our region is in the US, we can use a more accurate regional dataset USDA NASS Cropland Data Layers (CDL) to obtain a crop mask.\n\n\nShow Code\ncdl = (\n    ee.ImageCollection(\"USDA/NASS/CDL\")\n    .filter(ee.Filter.date(\"2022-01-01\", \"2023-01-01\"))\n    .first()\n)\ncropland = cdl.select(\"cropland\")\ncropland_mask = cdl.select(\"cultivated\").eq(2).rename(\"cropmask\")\n\n\n\n\nShow Code\nmap = geemap.Map(center=[40.04, -76.30], zoom=9)\nm.addLayer(\n    cropland_mask.clip(lancaster),\n    {\"min\": 0, \"max\": 1, \"palette\": [\"white\", \"green\"]},\n    \"Cropland Mask\",\n)\nm"
  },
  {
    "objectID": "satellite-embedding/demo2/landcover-classification.html#extract-training-samples",
    "href": "satellite-embedding/demo2/landcover-classification.html#extract-training-samples",
    "title": "Unsupervised Classification with Satellite Embedding Dataset",
    "section": "6 Extract training samples",
    "text": "6 Extract training samples\nWe apply the cropland mask to the embedding mosaic. We are now left with all the pixels representing cultivated cropland in the county.\n\n\nShow Code\ncluster_image = embeddingsImage.updateMask(cropland_mask).addBands(cropland_mask)\n\n\nWe need to take the Satellite Embedding image and obtain random samples to train a clustering model. Since our region of interest contains many masked pixels, a simple random sampling may result in samples with null values. To ensure we can extract the desired number of non-null samples, we use stratified sampling to obtain the desired number of samples in unmasked areas.\n\n\nShow Code\ntraining = cluster_image.stratifiedSample(\n    numPoints=1000,\n    classBand=\"cropmask\",\n    region=lancaster,\n    scale=10,\n    tileScale=16,\n    seed=100,\n    dropNulls=True,\n    geometries=True,\n)\n\n\n\n\nShow Code\nm.addLayer(training.style(**{\"color\": \"red\", \"pointSize\": 3}), {}, \"Training Points\")\nm"
  },
  {
    "objectID": "satellite-embedding/demo2/landcover-classification.html#perform-k-means-clustering",
    "href": "satellite-embedding/demo2/landcover-classification.html#perform-k-means-clustering",
    "title": "Unsupervised Classification with Satellite Embedding Dataset",
    "section": "7 Perform K-Means Clustering",
    "text": "7 Perform K-Means Clustering\nWe can now train a clusterer and group the 64D embedding vectors into a chosen number of distinct clusters. We can perform unsupervised clustering on the Satellite Embedding to obtain clusters of pixels that have similar temporal trajectories and patterns. Pixels with similar spectral and spatial characteristics along with similar phenology will be grouped in the same cluster.\nThe ee.Clusterer.wekaCascadeKMeans() allows us to specify a minimum and maximum number of clusters and find the optimal number of clusters based on the training data.\n\n\nShow Code\nmincluster = 4\nmaxcluster = 5\n\n\n\n\nShow Code\nclusterer = ee.Clusterer.wekaCascadeKMeans(\n    minClusters=mincluster,\n    maxClusters=maxcluster,\n).train(features=training, inputProperties=cluster_image.bandNames())\n\nclustered = cluster_image.cluster(clusterer)\n\n\n\n\nShow Code\nvis = clustered.randomVisualizer().clip(lancaster)\nm.addLayer(vis, {}, \"Clustered Image\")\nm\n\n\n\n\nShow Code\narea_image = ee.Image.pixelArea().divide(4046.86).addBands(clustered)\n\n\n\n\nShow Code\nareas = area_image.reduceRegion(\n    reducer=ee.Reducer.sum().group(\n        groupField=1,\n        groupName=\"cluster\",\n    ),\n    geometry=lancaster,\n    scale=10,\n    maxPixels=1e10,\n)\n\n\n\n\nShow Code\nprint(areas.getInfo())\n\n\n\n\nShow Code\ncluster_areas = ee.List(areas.get(\"groups\"))\n\n\n\n\nShow Code\nclusterAreas = ee.List(cluster_areas)\n\n\n\n\nShow Code\ndef to_feature(item):\n    d = ee.Dictionary(item)\n    return ee.Feature(\n        None, {\"cluster\": d.getNumber(\"cluster\").format(), \"area\": d.getNumber(\"sum\")}\n    )\n\n\ncluster_area_fc = ee.FeatureCollection(cluster_areas.map(to_feature))\n\n\n\n\nShow Code\nprint(cluster_area_fc.limit(10).getInfo())\n\n\nPrediction results (in acres): - Cluster 1: 25515.6423 - Cluster 2: 119071.5298 - Cluster 3: 62110.8848 - Cluster 4: 56379.1821\n\n7.1 Limitations of K-Means Clustering:\n\nWe need local knowledge to understand the optimal number of clusters to use for our analysis.\nWe also need local knowledge to figure out the land cover types represented by each cluster."
  },
  {
    "objectID": "satellite-embedding/demo2/landcover-classification.html#validating-classification-results",
    "href": "satellite-embedding/demo2/landcover-classification.html#validating-classification-results",
    "title": "Unsupervised Classification with Satellite Embedding Dataset",
    "section": "8 Validating classification results",
    "text": "8 Validating classification results\nBased on the USDA report, the main crops in Lancaster County are: - Corn for grain 95,549 + 35,988 = 131537; Prediction=119071 - Forage (hay/haylage), all 65,142 (others) - Soybeans for beans 51,695 - Wheat for grain, all 24,101\nWe try to group the crops into 3 clusters: Corn, Soybean, and Other crops for simplicity. \n\n\nShow Code\ncdl = (\n    ee.ImageCollection(\"USDA/NASS/CDL\")\n    .filter(ee.Filter.date(\"2022-01-01\", \"2023-01-01\"))\n    .first()\n)\ncropland = cdl.select(\"cropland\")\ncropmap = cropland.updateMask(cropland_mask).rename(\"crops\")\n\n\n\n\nShow Code\ncropclasses = ee.List.sequence(0, 254)\n\n\n\n\nShow Code\ntargetclasses = ee.List.repeat(0, 255).set(1, 1).set(5, 2)\n\n\n\n\nShow Code\ncropmapreclass = cropmap.remap(cropclasses, targetclasses).rename(\"crops\")\n\n\n\n\nShow Code\ncrop_vis = {\"min\": 0, \"max\": 2, \"palette\": [\"#bdbdbd\", \"#ffd400\", \"#267300\"]}\nm.addLayer(cropmapreclass.clip(lancaster), crop_vis, \"Reclassified Crop Map\")\nm\n\n\nValidation Results from USDA-NASS CDL 2022:\n\nUnsupervised classification results: from the Kmeans clustering on Satellite Embedding Dataset\n\nHighly suggest to run the code cells in the notebook to explore the interactive map to compare the classification results with the USDA-NASS CDL 2022 data."
  },
  {
    "objectID": "satellite-embedding/demo2/landcover-classification.html#references-and-acknowledgements",
    "href": "satellite-embedding/demo2/landcover-classification.html#references-and-acknowledgements",
    "title": "Unsupervised Classification with Satellite Embedding Dataset",
    "section": "9 References and acknowledgements",
    "text": "9 References and acknowledgements\n\nGoogle Earth Engine Satellite Embedding Tutorial by Ujaval Gandhi via official Google Earth Engine embedding tutorial: https://developers.google.com/earth-engine/tutorials/community/satellite-embedding-02-unsupervised-classification\nLeafmap package by Dr. Qiusheng Wu: https://leafmap.org/\ngeemap package by Dr. Qiusheng Wu: https://geemap.org/\nDr. Qiusheng Wu Google Satellite Embedding tutorial: https://www.youtube.com/watch?v=EGL7fXyA7-U\n\nThank you so much to Ujaval Gandhi and Dr. Qiusheng Wu for their amazing tutorials and open-source codes!"
  },
  {
    "objectID": "slides/slides.html#about-me",
    "href": "slides/slides.html#about-me",
    "title": "Satellite and Population Embedding",
    "section": "About Me",
    "text": "About Me\n\n\n\n\n\n\nZhanchao Yang\n\nMaster of City Planning (MCP)\nMaster of Urban Spatial Analytics (MUSA)\nBachelor of Arts in Geography, SUNY Binghamton\nResearch Assistant, Weitzman School of Design"
  },
  {
    "objectID": "slides/slides.html#what-is-embeddings",
    "href": "slides/slides.html#what-is-embeddings",
    "title": "Satellite and Population Embedding",
    "section": "What is Embeddings?",
    "text": "What is Embeddings?\n\nIn general terms, embeddings are numerical representations of complex information that capture the most important features or relationships in a simplified, machine-readable form."
  },
  {
    "objectID": "slides/slides.html#blackbox-nature-of-embeddings",
    "href": "slides/slides.html#blackbox-nature-of-embeddings",
    "title": "Satellite and Population Embedding",
    "section": "Blackbox Nature of Embeddings",
    "text": "Blackbox Nature of Embeddings\n\n\n\nGenerated through various neural network models (GNN or CNN) trained on a large datasets.\nExact architecture and training process vary widely.\nThe company or research institutions may not disclose the full details of their models."
  },
  {
    "objectID": "slides/slides.html#population-dynamics-foundation-pdfm",
    "href": "slides/slides.html#population-dynamics-foundation-pdfm",
    "title": "Satellite and Population Embedding",
    "section": "Population Dynamics Foundation (PDFM)",
    "text": "Population Dynamics Foundation (PDFM)\n\n\n\nDeveloped by Google Research (Google Deep Mind) in 2024.\nInput:\n\nCensus data\nSearching Trends\nOther geospatial data\n\nOutput:\n\nMulti-dimensional vectors (represent population characteristics and dynamics)."
  },
  {
    "objectID": "slides/slides.html#how-pdfm-embeddings-look-like",
    "href": "slides/slides.html#how-pdfm-embeddings-look-like",
    "title": "Satellite and Population Embedding",
    "section": "How PDFM Embeddings look like?",
    "text": "How PDFM Embeddings look like?\n\n\n\n330-dimensional vectors for each census tract or ZIP code\nEach dimension captures different aspects of population dynamics\nExamples of captured features:\n\nMobility patterns\nSearch behavior trends\nLocal economic activity\nEnvironmental conditions"
  },
  {
    "objectID": "slides/slides.html#applications-of-pdfm-embeddings",
    "href": "slides/slides.html#applications-of-pdfm-embeddings",
    "title": "Satellite and Population Embedding",
    "section": "Applications of PDFM Embeddings",
    "text": "Applications of PDFM Embeddings\n\n\nHealth & Social Services:\n\nDisease prevalence prediction\nHealthcare resource allocation\n\nEconomic Analysis:\n\nSocioeconomic indicators\nIncome and poverty estimation\n\nUrban Planning:\n\nPopulation growth forecasting\nPOI and hotspot prediction"
  },
  {
    "objectID": "slides/slides.html#additional-applications",
    "href": "slides/slides.html#additional-applications",
    "title": "Satellite and Population Embedding",
    "section": "Additional Applications",
    "text": "Additional Applications"
  },
  {
    "objectID": "slides/slides.html#demo-1-using-pdfm-embeddings-to-predict-housing-prices",
    "href": "slides/slides.html#demo-1-using-pdfm-embeddings-to-predict-housing-prices",
    "title": "Satellite and Population Embedding",
    "section": "Demo 1: Using PDFM Embeddings to predict housing prices",
    "text": "Demo 1: Using PDFM Embeddings to predict housing prices\nDataset:\n\nZillow Home Value Index (ZHVI) by ZIP code\nPDFM embeddings (326 features)\n\nModel:\n\nLinear Regression (Stepwise regression)\nData mining approaches\nOutput: Predicted home prices"
  },
  {
    "objectID": "slides/slides.html#satellite-foundation-model-sfm-embeddings",
    "href": "slides/slides.html#satellite-foundation-model-sfm-embeddings",
    "title": "Satellite and Population Embedding",
    "section": "Satellite Foundation Model (SFM) Embeddings",
    "text": "Satellite Foundation Model (SFM) Embeddings\n\n\n\nDeveloped by Google Research\nBased on Sentinel-2 satellite imagery\nGlobal coverage (2017-2023)\nSpatial resolution: 10 meters\nAvailable through Google Earth Engine (GEE)"
  },
  {
    "objectID": "slides/slides.html#what-satellite-embeddings-capture",
    "href": "slides/slides.html#what-satellite-embeddings-capture",
    "title": "Satellite and Population Embedding",
    "section": "What Satellite Embeddings Capture?",
    "text": "What Satellite Embeddings Capture?\n\n\n\nLand cover patterns\nVegetation characteristics\nUrban development\nEnvironmental changes\nTemporal dynamics\n\nTrained on massive satellite imagery corpus using self-supervised learning"
  },
  {
    "objectID": "slides/slides.html#satellite-embeddings-with-google-earth-engine-gee",
    "href": "slides/slides.html#satellite-embeddings-with-google-earth-engine-gee",
    "title": "Satellite and Population Embedding",
    "section": "Satellite Embeddings with Google Earth Engine (GEE)",
    "text": "Satellite Embeddings with Google Earth Engine (GEE)\nDataset:\n\nGOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL\n70 bands\n\nKey Operations:\n\nFilter by date and location\nExtract embeddings for points/regions\nCompare temporal changes\nCalculate similarity (dot product)"
  },
  {
    "objectID": "slides/slides.html#applications-of-sfm-embeddings",
    "href": "slides/slides.html#applications-of-sfm-embeddings",
    "title": "Satellite and Population Embedding",
    "section": "Applications of SFM Embeddings",
    "text": "Applications of SFM Embeddings\n\n\nEnvironmental and Land Use Planning\n\nUrban sprawl analysis\nLand cover classification\n\nTemporal Analysis:\n\nSeasonal pattern detection\nChange Detection\nSimilarity searches across time"
  },
  {
    "objectID": "slides/slides.html#demo-2-satellite-embeddings-for-land-cover-classification",
    "href": "slides/slides.html#demo-2-satellite-embeddings-for-land-cover-classification",
    "title": "Satellite and Population Embedding",
    "section": "Demo 2: Satellite Embeddings for Land Cover Classification",
    "text": "Demo 2: Satellite Embeddings for Land Cover Classification\n\n\nApproach:\n\nUse embeddings as features\nTrain classification model\nPredict land cover types\n\nAdvantages:\n\nNo need to process raw imagery\nPre-trained representations\nFaster than traditional methods"
  },
  {
    "objectID": "slides/slides.html#workflow",
    "href": "slides/slides.html#workflow",
    "title": "Satellite and Population Embedding",
    "section": "Workflow",
    "text": "Workflow\n\nDefine area of interest\nLoad embedding ImageCollection\nExtract features for labeled samples\nTrain classifier (Random Forest, SVM)\nApply to entire region\nValidate results\n\nUses Earth Engine’s cloud computing for scalability"
  },
  {
    "objectID": "slides/slides.html#limitations-and-concerns",
    "href": "slides/slides.html#limitations-and-concerns",
    "title": "Satellite and Population Embedding",
    "section": "Limitations and Concerns",
    "text": "Limitations and Concerns\nInterpretability Issues:\n\nBlack box representations\nLimited transparency in training process\n\nBias and Fairness:\n\nMay encode historical biases\nRisk of perpetuating spatial inequalities\n\nTechnical Limitations:\n\nResolution constraints (10m for satellite)\nUpdates not in real-time\n\n…"
  },
  {
    "objectID": "slides/slides.html#summary-and-conclusions",
    "href": "slides/slides.html#summary-and-conclusions",
    "title": "Satellite and Population Embedding",
    "section": "Summary and Conclusions",
    "text": "Summary and Conclusions\n\n\nThe Promise:\n\nPowerful tools for spatial analysis\nCapture complex patterns\n\nThe Reality:\n\nRequire careful validation\nComplement, not replace\n\nBest Practices:\n\nValidate predictions thoroughly\nBe aware of limitations and biases"
  },
  {
    "objectID": "slides/slides.html#final-verdict",
    "href": "slides/slides.html#final-verdict",
    "title": "Satellite and Population Embedding",
    "section": "Final Verdict",
    "text": "Final Verdict\n \n\nPowerful GeoAI tools, but use with caution and critical thinking!\n\n\n“All models are false, but some are useful”"
  },
  {
    "objectID": "slides/slides.html#references",
    "href": "slides/slides.html#references",
    "title": "Satellite and Population Embedding",
    "section": "References",
    "text": "References\n\nGoogle Research:\n\nGoogle PDFM Embeddings: github.com/google-research/population-dynamics\nSatellite Embeddings Dataset: developers.google.com/earth-engine/datasets/catalog/GOOGLE_SATELLITE_EMBEDDING_V1_ANNUAL\n\nTutorials & Resources:\n\nDr. Qiusheng Wu PDFM Tutorial: github.com/opengeos/GeoAI-Tutorials\nDr. Qiusheng Wu Earth Embedding Tutorial: leafmap.org/maplibre/AlphaEarth\nGoogle Earth Engine Tutorial: developers.google.com/earth-engine/tutorials/community/satellite-embedding-02-unsupervised-classification\n\nThis Presentation:\n\nGitHub Repository: github.com/zyang91/Google-Embedding-tutorial"
  },
  {
    "objectID": "pdfm/demo1/demo1.html",
    "href": "pdfm/demo1/demo1.html",
    "title": "Zillow Home Value Index Analysis with PDFM Embeddings",
    "section": "",
    "text": "This document demonstrates the application of Population Dynamics Foundation Model (PDFM) embeddings to predict Zillow Home Value Index (ZHVI) data. PDFM embeddings are 330-dimensional vector representations that capture complex spatial and demographic patterns.\n\n\n\nLoad and visualize Zillow Home Value Index data with geospatial mapping\nJoin PDFM embeddings with home value data\nBuild regression models to predict home values:\n\nLinear Regression (baseline)\nRidge Regression (L2 regularization)\nLasso Regression (L1 regularization with feature selection)\n\nEvaluate and visualize model performance\n\n\n\n\nRidge and Lasso regression are particularly useful when working with high-dimensional embeddings (330 features) because they:\n\nRidge Regression (L2):\n\nPenalizes large coefficients to prevent overfitting\nKeeps all features but shrinks their impact\nPerforms well when many features contribute to the outcome\n\nLasso Regression (L1):\n\nPerforms automatic feature selection by shrinking some coefficients to zero\nIdentifies the most important embedding dimensions\nProduces sparse models that are easier to interpret\n\nBoth:\n\nUse cross-validation to optimize the regularization parameter (lambda)\nMore computationally efficient than stepwise regression\nProvide better generalization on unseen data"
  },
  {
    "objectID": "pdfm/demo1/demo1.html#introduction",
    "href": "pdfm/demo1/demo1.html#introduction",
    "title": "Zillow Home Value Index Analysis with PDFM Embeddings",
    "section": "",
    "text": "This document demonstrates the application of Population Dynamics Foundation Model (PDFM) embeddings to predict Zillow Home Value Index (ZHVI) data. PDFM embeddings are 330-dimensional vector representations that capture complex spatial and demographic patterns.\n\n\n\nLoad and visualize Zillow Home Value Index data with geospatial mapping\nJoin PDFM embeddings with home value data\nBuild regression models to predict home values:\n\nLinear Regression (baseline)\nRidge Regression (L2 regularization)\nLasso Regression (L1 regularization with feature selection)\n\nEvaluate and visualize model performance\n\n\n\n\nRidge and Lasso regression are particularly useful when working with high-dimensional embeddings (330 features) because they:\n\nRidge Regression (L2):\n\nPenalizes large coefficients to prevent overfitting\nKeeps all features but shrinks their impact\nPerforms well when many features contribute to the outcome\n\nLasso Regression (L1):\n\nPerforms automatic feature selection by shrinking some coefficients to zero\nIdentifies the most important embedding dimensions\nProduces sparse models that are easier to interpret\n\nBoth:\n\nUse cross-validation to optimize the regularization parameter (lambda)\nMore computationally efficient than stepwise regression\nProvide better generalization on unseen data"
  },
  {
    "objectID": "pdfm/demo1/demo1.html#setup-and-data-loading",
    "href": "pdfm/demo1/demo1.html#setup-and-data-loading",
    "title": "Zillow Home Value Index Analysis with PDFM Embeddings",
    "section": "Setup and Data Loading",
    "text": "Setup and Data Loading\n\nLoad Required Libraries\n\n\nCode\n# Data manipulation and analysis\nlibrary(tidyverse)\nlibrary(readr)\n\n# Geospatial data handling\nlibrary(sf)\nlibrary(leaflet)\n\n# Machine learning and modeling\nlibrary(caret)\nlibrary(MASS)  # For stepwise regression\nlibrary(glmnet)  # For Ridge and Lasso regression\n\n# Model evaluation\nlibrary(Metrics)\nlibrary(ggplot2)\n\n# For table formatting\nlibrary(knitr)\nlibrary(kableExtra)\n\n# Set random seed for reproducibility\nset.seed(42)\n\n\n\n\nDownload Zillow Home Value Index Data\n\n\nCode\n# Download ZHVI data\nzhvi &lt;- read.csv(\"https://github.com/opengeos/datasets/releases/download/us/zillow_home_value_index_by_county.csv\")\n\n\n\n\nLoad and Prepare ZHVI Data\n\n\nCode\n# constuct correct State FIPS code and Municipal FIPS code with leading zeros\nzhvi_df &lt;- zhvi %&gt;%\n  mutate(\n    StateCodeFIP = str_pad(as.character(StateCodeFIPS), width = 2, side = \"left\", pad = \"0\"),\n    MunicipalCodeFIP = str_pad(as.character(MunicipalCodeFIPS), width = 3, side = \"left\", pad = \"0\")\n  )\n# Create place identifier\nzhvi_df &lt;- zhvi_df %&gt;%\n  mutate(\n    place = paste0(\"geoId/\", StateCodeFIP, MunicipalCodeFIP)\n  )\n\n\nNote: The place column creates a unique identifier for each county by combining state and municipal FIPS codes, which will be used to join with geospatial and embedding data."
  },
  {
    "objectID": "pdfm/demo1/demo1.html#geospatial-data-integration",
    "href": "pdfm/demo1/demo1.html#geospatial-data-integration",
    "title": "Zillow Home Value Index Analysis with PDFM Embeddings",
    "section": "Geospatial Data Integration",
    "text": "Geospatial Data Integration\n\nLoad County Geometries\n\n\nCode\ncounty_gdf &lt;- st_read(\"https://github.com/zyang91/Google-Embedding-tutorial/releases/download/v2.0.0/county.geojson\",quiet=TRUE)\n\ncouty_gdf &lt;- county_gdf %&gt;%\n  dplyr::select(place)\n\n\n\n\nJoin ZHVI with County Geometries\n\n\nCode\nzhvi_county_gdf&lt;- county_gdf %&gt;%\n  inner_join(\n    zhvi_df,\n    by = c(\"place\" = \"place\")\n  )"
  },
  {
    "objectID": "pdfm/demo1/demo1.html#visualizing-home-values",
    "href": "pdfm/demo1/demo1.html#visualizing-home-values",
    "title": "Zillow Home Value Index Analysis with PDFM Embeddings",
    "section": "Visualizing Home Values",
    "text": "Visualizing Home Values\n\nPrepare Data for Visualization\n\n\nCode\n# Select specific date column for visualization\ntarget_date &lt;- \"X2024.10.31\"\nviz_gdf &lt;- zhvi_county_gdf %&gt;%\n  dplyr::select(RegionName, State, all_of(target_date), geometry)\n\n\n\n\nCreate 2D Choropleth Map\n\n\nCode\n# Create interactive map with Leaflet\npal &lt;- colorNumeric(\n  palette = \"Blues\",\n  domain = viz_gdf[[target_date]],\n  na.color = \"transparent\"\n)\n\nleaflet(viz_gdf) %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    fillColor = ~pal(get(target_date)),\n    fillOpacity = 0.7,\n    color = \"white\",\n    weight = 1,\n    popup = ~paste0(\n      \"&lt;strong&gt;\", RegionName, \", \", State, \"&lt;/strong&gt;&lt;br&gt;\",\n      \"Home Value: $\", format(get(target_date), big.mark = \",\")\n    )\n  ) %&gt;%\n  addLegend(\n    position = \"bottomright\",\n    pal = pal,\n    values = ~get(target_date),\n    title = \"Zillow Home Median Value\",\n    opacity = 1\n  )\n\n\n\n\n\n\nNote: This creates an interactive map where users can hover over counties to see home values. The blue color gradient represents the magnitude of home values."
  },
  {
    "objectID": "pdfm/demo1/demo1.html#pdfm-embeddings-integration",
    "href": "pdfm/demo1/demo1.html#pdfm-embeddings-integration",
    "title": "Zillow Home Value Index Analysis with PDFM Embeddings",
    "section": "PDFM Embeddings Integration",
    "text": "PDFM Embeddings Integration\n\nLoad PDFM Embeddings\n\n\nCode\n# Load pre-computed PDFM embeddings\nembeddings &lt;- read_csv(\"https://github.com/zyang91/Google-Embedding-tutorial/releases/download/v2.0.0/county_embeddings.csv\")\n\n\nAbout PDFM Embeddings: These 330-dimensional vectors encode complex spatial patterns including:\n\nPopulation mobility patterns\nSearch behavior trends\nLocal economic activity indicators\nEnvironmental conditions\nDemographic characteristics\n\n\n\nVisualize Single Embedding Feature\n\n\nCode\n# Join embeddings with county geometries\ndf_embed &lt;- county_gdf %&gt;%\n  inner_join(embeddings,\n    by = \"place\"\n  )\n\n# Select one embedding feature to visualize\nfeature_col &lt;- \"feature329\"\nviz_embed &lt;- df_embed %&gt;%\n  dplyr::select(state, all_of(feature_col), geometry)\n\n\n\n\nCode\n# Create map\npal_embed &lt;- colorNumeric(\n  palette = \"Blues\",\n  domain = viz_embed[[feature_col]],\n  na.color = \"transparent\"\n)\n\nleaflet(viz_embed) %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    fillColor = ~pal_embed(get(feature_col)),\n    fillOpacity = 0.7,\n    color = \"white\",\n    weight = 1,\n    popup = ~paste0(\n      \"&lt;strong&gt;\", state, \"&lt;/strong&gt;&lt;br&gt;\",\n      feature_col, \": \", round(get(feature_col), 4)\n    )\n  ) %&gt;%\n  addLegend(\n    position = \"bottomright\",\n    pal = pal_embed,\n    values = ~get(feature_col),\n    title = feature_col,\n    opacity = 1\n  )\n\n\n\n\n\n\nNote: Each of the 330 embedding features captures different spatial patterns. Feature329 is visualized here as an example."
  },
  {
    "objectID": "pdfm/demo1/demo1.html#regression-modeling",
    "href": "pdfm/demo1/demo1.html#regression-modeling",
    "title": "Zillow Home Value Index Analysis with PDFM Embeddings",
    "section": "Regression Modeling",
    "text": "Regression Modeling\n\nPrepare Training Data\n\n\nCode\n# Join ZHVI with embeddings\ndata &lt;- zhvi_df %&gt;%\n  inner_join(\n    embeddings,\n    by = \"place\"\n  )\n\n# Define embedding features and target variable\nembedding_features &lt;- paste0(\"feature\", 0:329)\ntarget_label &lt;- \"X2024.10.31\"\n\n# Remove rows with missing target values\ndata &lt;- data %&gt;%\n  filter(!is.na(get(target_label)))\n\n# Select only features and target for modeling\nmodeling_data &lt;- data %&gt;%\n  dplyr::select(all_of(c(embedding_features, target_label)))\nmodeling_data &lt;- modeling_data %&gt;%\n  mutate(index = row_number())\n# Split into training and testing sets (80/20 split)\ntrain_indices &lt;- createDataPartition(modeling_data$index, p = 0.8, list = FALSE)\ntrain_data &lt;- modeling_data[train_indices, ]\ntest_data &lt;- modeling_data[-train_indices, ]\n\n\nTest data:604 Train data:2431\n\n\nModel 1: Linear Regression (Baseline)\n\n\nCode\n# refine train data\ntrain_data &lt;- train_data %&gt;%\n  dplyr::select(-index)\n\ntrain_data&lt;- train_data %&gt;%\n  rename(target = X2024.10.31)\n\n# Fit linear regression model using all features\nlr_model &lt;- lm(target ~ ., data = train_data)\n\nsummary(lr_model)\n\n\n\nCall:\nlm(formula = target ~ ., data = train_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-376020  -29025    -988   27176  758839 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.923e+05  3.417e+04   8.554  &lt; 2e-16 ***\nfeature0    -6.484e+04  4.664e+04  -1.390 0.164591    \nfeature1    -1.890e+04  5.206e+03  -3.629 0.000291 ***\nfeature2    -9.262e+03  6.099e+03  -1.519 0.129003    \nfeature3    -1.944e+02  5.940e+03  -0.033 0.973901    \nfeature4    -1.000e+03  5.509e+03  -0.182 0.855949    \nfeature5    -3.751e+03  4.974e+03  -0.754 0.450865    \nfeature6    -5.508e+03  5.543e+03  -0.994 0.320564    \nfeature7    -1.079e+04  5.613e+03  -1.922 0.054740 .  \nfeature8    -2.761e+03  5.373e+03  -0.514 0.607335    \nfeature9    -1.680e+04  5.227e+03  -3.215 0.001326 ** \nfeature10    1.849e+03  5.412e+03   0.342 0.732617    \nfeature11    2.269e+03  5.422e+03   0.418 0.675678    \nfeature12    2.725e+04  5.481e+03   4.971 7.19e-07 ***\nfeature13    2.693e+04  5.759e+03   4.677 3.09e-06 ***\nfeature14    2.102e+03  5.674e+03   0.370 0.711086    \nfeature15    2.586e+04  5.688e+03   4.545 5.80e-06 ***\nfeature16    1.292e+04  6.260e+03   2.064 0.039172 *  \nfeature17   -1.871e+04  4.968e+03  -3.765 0.000171 ***\nfeature18   -1.067e+04  5.602e+03  -1.905 0.056909 .  \nfeature19   -2.534e+04  5.272e+03  -4.806 1.65e-06 ***\nfeature20   -6.973e+03  4.941e+03  -1.411 0.158381    \nfeature21   -1.663e+04  5.008e+03  -3.321 0.000912 ***\nfeature22    9.916e+03  5.768e+03   1.719 0.085748 .  \nfeature23    3.453e+04  5.701e+03   6.056 1.65e-09 ***\nfeature24   -1.408e+04  4.706e+04  -0.299 0.764823    \nfeature25    2.313e+03  5.249e+03   0.441 0.659473    \nfeature26   -7.118e+02  5.816e+03  -0.122 0.902602    \nfeature27    1.906e+03  5.959e+03   0.320 0.749089    \nfeature28    1.033e+04  6.071e+03   1.701 0.089122 .  \nfeature29    1.481e+04  5.775e+03   2.564 0.010431 *  \nfeature30   -6.455e+03  5.632e+03  -1.146 0.251873    \nfeature31   -3.879e+02  5.539e+03  -0.070 0.944175    \nfeature32    6.302e+03  5.052e+03   1.247 0.212370    \nfeature33    3.139e+04  5.408e+03   5.803 7.49e-09 ***\nfeature34   -3.321e+03  5.220e+03  -0.636 0.524703    \nfeature35    2.918e+04  6.084e+03   4.797 1.73e-06 ***\nfeature36    1.710e+03  5.249e+03   0.326 0.744653    \nfeature37   -5.855e+03  5.742e+03  -1.020 0.308033    \nfeature38   -1.128e+04  5.782e+03  -1.951 0.051186 .  \nfeature39   -2.087e+03  4.609e+03  -0.453 0.650734    \nfeature40   -1.478e+04  4.830e+03  -3.060 0.002242 ** \nfeature41   -2.229e+04  5.793e+03  -3.848 0.000123 ***\nfeature42   -9.822e+03  5.209e+03  -1.886 0.059497 .  \nfeature43   -3.839e+03  5.325e+03  -0.721 0.471002    \nfeature44    2.298e+04  5.672e+03   4.052 5.26e-05 ***\nfeature45   -1.500e+04  5.647e+03  -2.656 0.007962 ** \nfeature46   -9.565e+02  5.391e+03  -0.177 0.859182    \nfeature47    9.350e+03  5.738e+03   1.630 0.103354    \nfeature48   -2.983e+03  5.205e+03  -0.573 0.566709    \nfeature49   -4.090e+03  5.177e+03  -0.790 0.429584    \nfeature50    1.425e+04  5.577e+03   2.555 0.010682 *  \nfeature51    5.759e+03  5.474e+03   1.052 0.292947    \nfeature52   -1.329e+04  5.300e+03  -2.508 0.012231 *  \nfeature53    1.104e+04  5.498e+03   2.008 0.044747 *  \nfeature54   -4.584e+03  5.339e+03  -0.859 0.390698    \nfeature55   -1.421e+03  5.824e+03  -0.244 0.807308    \nfeature56   -9.396e+03  5.481e+03  -1.714 0.086634 .  \nfeature57   -3.887e+04  5.274e+03  -7.371 2.42e-13 ***\nfeature58   -6.546e+03  5.264e+03  -1.244 0.213803    \nfeature59   -1.552e+04  5.285e+03  -2.937 0.003347 ** \nfeature60    1.546e+04  5.352e+03   2.888 0.003912 ** \nfeature61    1.253e+04  5.852e+03   2.140 0.032442 *  \nfeature62    7.411e+02  5.660e+03   0.131 0.895836    \nfeature63    3.691e+03  5.993e+03   0.616 0.538064    \nfeature64    2.213e+03  5.341e+03   0.414 0.678596    \nfeature65   -2.288e+04  5.608e+03  -4.080 4.67e-05 ***\nfeature66   -1.797e+04  5.094e+03  -3.528 0.000428 ***\nfeature67    1.293e+04  5.624e+03   2.299 0.021606 *  \nfeature68   -1.094e+04  5.291e+03  -2.068 0.038740 *  \nfeature69    5.633e+03  5.360e+03   1.051 0.293468    \nfeature70    3.634e+03  5.024e+03   0.723 0.469615    \nfeature71   -2.033e+04  5.520e+03  -3.683 0.000236 ***\nfeature72    1.429e+03  5.157e+03   0.277 0.781714    \nfeature73   -1.893e+04  5.822e+03  -3.252 0.001166 ** \nfeature74   -1.495e+04  5.001e+03  -2.989 0.002830 ** \nfeature75   -3.786e+03  5.072e+03  -0.747 0.455404    \nfeature76    1.450e+05  4.749e+04   3.053 0.002293 ** \nfeature77    4.332e+04  5.854e+03   7.399 1.97e-13 ***\nfeature78    1.631e+04  5.466e+03   2.984 0.002877 ** \nfeature79    2.559e+04  5.962e+03   4.292 1.85e-05 ***\nfeature80   -1.601e+03  5.732e+03  -0.279 0.780022    \nfeature81    3.645e+04  5.842e+03   6.240 5.28e-10 ***\nfeature82    3.046e+04  5.481e+03   5.558 3.08e-08 ***\nfeature83   -4.520e+03  5.843e+03  -0.774 0.439251    \nfeature84   -1.994e+04  5.488e+03  -3.634 0.000286 ***\nfeature85    2.423e+04  5.692e+03   4.257 2.17e-05 ***\nfeature86    1.020e+04  6.270e+03   1.626 0.104003    \nfeature87   -3.117e+04  5.528e+03  -5.638 1.95e-08 ***\nfeature88   -8.138e+03  5.295e+03  -1.537 0.124499    \nfeature89   -1.740e+04  4.782e+03  -3.638 0.000281 ***\nfeature90   -3.430e+03  6.131e+03  -0.559 0.575896    \nfeature91    3.688e+04  5.434e+03   6.788 1.47e-11 ***\nfeature92   -1.167e+04  5.197e+03  -2.246 0.024787 *  \nfeature93   -3.999e+03  5.586e+03  -0.716 0.474151    \nfeature94    3.826e+04  5.701e+03   6.712 2.46e-11 ***\nfeature95   -3.883e+04  6.359e+03  -6.107 1.21e-09 ***\nfeature96   -1.535e+04  5.856e+03  -2.621 0.008823 ** \nfeature97    2.035e+03  5.889e+03   0.346 0.729740    \nfeature98   -1.053e+04  5.896e+03  -1.787 0.074162 .  \nfeature99    7.540e+03  5.979e+03   1.261 0.207426    \nfeature100  -1.756e+04  5.719e+03  -3.070 0.002167 ** \nfeature101   1.548e+04  5.441e+03   2.846 0.004473 ** \nfeature102   1.352e+04  5.656e+03   2.390 0.016946 *  \nfeature103   2.439e+04  5.725e+03   4.260 2.14e-05 ***\nfeature104  -2.295e+04  5.329e+03  -4.306 1.74e-05 ***\nfeature105  -1.771e+03  5.465e+03  -0.324 0.745878    \nfeature106  -7.883e+03  5.229e+03  -1.508 0.131805    \nfeature107  -1.385e+04  5.449e+03  -2.542 0.011097 *  \nfeature108   4.519e+04  6.781e+03   6.664 3.39e-11 ***\nfeature109  -1.057e+04  5.632e+03  -1.877 0.060636 .  \nfeature110   4.495e+03  5.690e+03   0.790 0.429661    \nfeature111   2.867e+04  5.947e+03   4.821 1.53e-06 ***\nfeature112   5.693e+03  5.667e+03   1.005 0.315212    \nfeature113  -4.713e+03  5.597e+03  -0.842 0.399782    \nfeature114   4.099e+03  5.926e+03   0.692 0.489179    \nfeature115  -4.339e+03  5.502e+03  -0.789 0.430451    \nfeature116   6.155e+03  5.765e+03   1.068 0.285803    \nfeature117  -5.910e+03  4.294e+03  -1.376 0.168814    \nfeature118   1.899e+03  5.149e+03   0.369 0.712390    \nfeature119  -3.195e+03  5.244e+03  -0.609 0.542345    \nfeature120  -1.044e+03  5.787e+03  -0.180 0.856806    \nfeature121  -6.598e+03  5.689e+03  -1.160 0.246281    \nfeature122  -3.457e+04  6.160e+03  -5.613 2.25e-08 ***\nfeature123  -4.675e+03  5.486e+03  -0.852 0.394148    \nfeature124  -2.770e+04  5.094e+03  -5.437 6.06e-08 ***\nfeature125   3.368e+04  6.147e+03   5.478 4.81e-08 ***\nfeature126   1.053e+04  5.869e+03   1.793 0.073063 .  \nfeature127   2.075e+04  5.508e+03   3.767 0.000170 ***\nfeature128  -2.126e+01  4.490e+03  -0.005 0.996223    \nfeature129   1.579e+04  1.184e+04   1.333 0.182739    \nfeature130  -5.363e+03  4.978e+03  -1.077 0.281466    \nfeature131  -3.811e+03  3.341e+03  -1.141 0.254195    \nfeature132  -1.194e+04  8.623e+03  -1.385 0.166323    \nfeature133   5.748e+03  5.745e+03   1.000 0.317201    \nfeature134  -2.098e+03  7.242e+04  -0.029 0.976895    \nfeature135   7.097e+03  1.288e+04   0.551 0.581683    \nfeature136  -2.899e+03  1.341e+04  -0.216 0.828872    \nfeature137  -8.275e+04  7.399e+04  -1.118 0.263526    \nfeature138   1.743e+05  6.397e+04   2.725 0.006479 ** \nfeature139   5.676e+04  6.966e+04   0.815 0.415254    \nfeature140   4.231e+04  1.256e+04   3.369 0.000768 ***\nfeature141  -9.457e+03  1.035e+04  -0.913 0.361210    \nfeature142   5.172e+03  6.141e+03   0.842 0.399777    \nfeature143  -9.761e+03  9.786e+03  -0.997 0.318652    \nfeature144   5.040e+04  1.237e+04   4.073 4.81e-05 ***\nfeature145  -1.329e+05  5.913e+04  -2.247 0.024725 *  \nfeature146   1.159e+04  1.070e+04   1.082 0.279169    \nfeature147  -9.822e+02  4.805e+03  -0.204 0.838074    \nfeature148   1.240e+04  7.181e+03   1.727 0.084338 .  \nfeature149   4.676e+02  5.855e+03   0.080 0.936350    \nfeature150   6.264e+04  5.951e+04   1.053 0.292622    \nfeature151   4.345e+01  3.146e+03   0.014 0.988980    \nfeature152  -8.972e+04  6.556e+04  -1.368 0.171318    \nfeature153  -4.021e+04  1.546e+04  -2.601 0.009357 ** \nfeature154  -2.750e+02  1.146e+04  -0.024 0.980862    \nfeature155   9.481e+03  6.966e+03   1.361 0.173658    \nfeature156   2.138e+03  7.523e+03   0.284 0.776314    \nfeature157   1.342e+04  1.204e+04   1.114 0.265277    \nfeature158  -9.103e+08  3.537e+09  -0.257 0.796893    \nfeature159  -2.136e+04  1.403e+04  -1.522 0.128133    \nfeature160  -3.596e+03  3.839e+03  -0.937 0.348997    \nfeature161  -7.898e+01  6.365e+03  -0.012 0.990102    \nfeature162  -1.023e+04  1.032e+04  -0.991 0.321875    \nfeature163   5.567e+04  5.363e+04   1.038 0.299349    \nfeature164  -7.599e+03  3.745e+03  -2.029 0.042607 *  \nfeature165  -2.290e+04  7.699e+03  -2.974 0.002972 ** \nfeature166   9.422e+02  5.764e+04   0.016 0.986959    \nfeature167   1.304e+04  5.708e+04   0.228 0.819362    \nfeature168   1.001e+05  6.673e+04   1.500 0.133863    \nfeature169   2.110e+05  6.573e+04   3.210 0.001348 ** \nfeature170   1.004e+05  5.518e+04   1.820 0.068971 .  \nfeature171  -1.818e+04  1.071e+04  -1.698 0.089747 .  \nfeature172  -1.067e+05  6.661e+04  -1.601 0.109450    \nfeature173  -1.254e+04  5.471e+03  -2.292 0.021981 *  \nfeature174  -6.760e+03  6.325e+03  -1.069 0.285276    \nfeature175   1.317e+05  7.704e+04   1.710 0.087441 .  \nfeature176  -2.983e+03  4.591e+03  -0.650 0.516003    \nfeature177   1.624e+03  2.954e+03   0.550 0.582632    \nfeature178   1.114e+04  1.027e+04   1.085 0.278185    \nfeature179   1.347e+03  7.355e+03   0.183 0.854706    \nfeature180  -3.947e+03  9.036e+03  -0.437 0.662332    \nfeature181  -2.386e+04  6.382e+03  -3.739 0.000190 ***\nfeature182  -4.717e+03  8.251e+03  -0.572 0.567602    \nfeature183   1.562e+03  3.317e+03   0.471 0.637671    \nfeature184   1.546e+05  6.194e+04   2.496 0.012626 *  \nfeature185  -7.612e+02  5.275e+03  -0.144 0.885278    \nfeature186   5.186e+03  5.944e+03   0.873 0.382994    \nfeature187   1.887e+04  9.275e+03   2.035 0.041977 *  \nfeature188  -2.257e+03  5.644e+03  -0.400 0.689314    \nfeature189   2.148e+03  5.713e+03   0.376 0.707012    \nfeature190  -6.345e+03  1.668e+04  -0.380 0.703668    \nfeature191  -9.327e+04  5.730e+04  -1.628 0.103732    \nfeature192   9.891e+04  8.122e+04   1.218 0.223458    \nfeature193   7.396e+03  6.077e+03   1.217 0.223749    \nfeature194  -1.573e+05  5.810e+04  -2.708 0.006824 ** \nfeature195   6.023e+03  5.212e+03   1.156 0.247937    \nfeature196   2.299e+05  7.168e+04   3.208 0.001358 ** \nfeature197   4.481e+04  5.742e+04   0.780 0.435231    \nfeature198  -1.499e+02  9.767e+03  -0.015 0.987756    \nfeature199  -7.827e+04  5.342e+04  -1.465 0.143052    \nfeature200   3.919e+03  6.045e+03   0.648 0.516842    \nfeature201  -1.371e+04  1.810e+04  -0.757 0.448931    \nfeature202   1.300e+04  1.412e+04   0.921 0.357398    \nfeature203  -1.620e+04  5.272e+04  -0.307 0.758642    \nfeature204  -1.700e+03  7.334e+03  -0.232 0.816778    \nfeature205  -2.272e+04  5.860e+04  -0.388 0.698301    \nfeature206   9.101e+04  6.805e+04   1.337 0.181262    \nfeature207   5.414e+04  8.522e+03   6.352 2.59e-10 ***\nfeature208   1.615e+04  6.543e+04   0.247 0.805118    \nfeature209  -4.616e+04  1.335e+04  -3.458 0.000555 ***\nfeature210  -6.910e+03  9.694e+03  -0.713 0.476057    \nfeature211   2.386e+03  1.503e+04   0.159 0.873891    \nfeature212  -3.547e+04  1.527e+04  -2.323 0.020285 *  \nfeature213  -1.713e+05  5.824e+04  -2.941 0.003312 ** \nfeature214  -3.778e+03  5.138e+03  -0.735 0.462259    \nfeature215  -1.127e+03  5.238e+03  -0.215 0.829712    \nfeature216  -3.234e+04  6.384e+04  -0.507 0.612515    \nfeature217  -9.149e+03  4.123e+03  -2.219 0.026602 *  \nfeature218  -2.571e+04  1.028e+04  -2.501 0.012444 *  \nfeature219   8.461e+03  1.403e+04   0.603 0.546586    \nfeature220   3.763e+03  1.013e+04   0.371 0.710341    \nfeature221  -1.667e+04  2.381e+04  -0.700 0.483959    \nfeature222   2.652e+04  1.241e+04   2.136 0.032775 *  \nfeature223   7.303e+04  6.008e+04   1.215 0.224340    \nfeature224  -7.310e+04  8.271e+04  -0.884 0.376864    \nfeature225  -3.368e+04  1.483e+04  -2.272 0.023210 *  \nfeature226  -1.314e+05  5.742e+04  -2.289 0.022204 *  \nfeature227  -8.502e+03  1.225e+04  -0.694 0.487563    \nfeature228  -9.595e+03  7.501e+03  -1.279 0.200999    \nfeature229   8.698e+03  8.375e+04   0.104 0.917285    \nfeature230   8.051e+04  1.557e+04   5.172 2.53e-07 ***\nfeature231   1.781e+04  1.614e+04   1.103 0.270096    \nfeature232   1.110e+02  6.524e+03   0.017 0.986421    \nfeature233  -2.238e+04  2.118e+04  -1.056 0.290870    \nfeature234   2.052e+04  1.825e+04   1.124 0.261094    \nfeature235  -1.374e+04  4.907e+03  -2.799 0.005166 ** \nfeature236  -2.629e+03  9.922e+03  -0.265 0.791029    \nfeature237  -4.339e+03  4.707e+03  -0.922 0.356798    \nfeature238   3.852e+03  6.369e+04   0.060 0.951786    \nfeature239  -1.894e+04  1.028e+04  -1.843 0.065541 .  \nfeature240  -8.501e+03  5.459e+03  -1.557 0.119549    \nfeature241   5.802e+03  6.435e+03   0.902 0.367386    \nfeature242  -6.039e+03  7.393e+03  -0.817 0.414139    \nfeature243   3.022e+04  6.481e+04   0.466 0.641033    \nfeature244   2.110e+04  1.107e+04   1.906 0.056781 .  \nfeature245   3.703e+04  5.322e+04   0.696 0.486665    \nfeature246  -6.135e+03  8.861e+03  -0.692 0.488770    \nfeature247  -1.546e+05  6.139e+04  -2.518 0.011892 *  \nfeature248   4.183e+03  5.065e+03   0.826 0.408944    \nfeature249   1.015e+04  1.532e+04   0.663 0.507507    \nfeature250   3.404e+04  1.347e+04   2.527 0.011570 *  \nfeature251  -4.872e+03  9.242e+03  -0.527 0.598160    \nfeature252  -3.918e+04  1.377e+04  -2.845 0.004491 ** \nfeature253   1.164e+05  6.168e+04   1.888 0.059203 .  \nfeature254  -1.729e+05  7.181e+04  -2.407 0.016154 *  \nfeature255  -9.907e+03  6.626e+04  -0.150 0.881151    \nfeature256   1.004e+03  2.544e+03   0.394 0.693312    \nfeature257   3.717e+03  3.254e+03   1.142 0.253439    \nfeature258  -2.176e+03  3.304e+03  -0.659 0.510255    \nfeature259   1.809e+03  3.847e+03   0.470 0.638314    \nfeature260   7.726e+02  3.490e+03   0.221 0.824835    \nfeature261  -1.868e+03  2.728e+03  -0.685 0.493494    \nfeature262  -8.123e+03  3.373e+03  -2.408 0.016105 *  \nfeature263   8.999e+03  3.941e+03   2.283 0.022506 *  \nfeature264  -6.545e+03  3.539e+03  -1.850 0.064503 .  \nfeature265   3.401e+03  3.532e+03   0.963 0.335796    \nfeature266  -8.592e+03  3.202e+03  -2.684 0.007340 ** \nfeature267   5.477e+04  5.483e+04   0.999 0.317957    \nfeature268   9.102e+02  3.822e+03   0.238 0.811766    \nfeature269  -9.589e+03  3.967e+03  -2.417 0.015731 *  \nfeature270   4.323e+03  3.908e+03   1.106 0.268770    \nfeature271   1.410e+03  2.833e+03   0.498 0.618650    \nfeature272   3.846e+03  3.838e+03   1.002 0.316448    \nfeature273   1.772e+03  3.687e+03   0.481 0.630913    \nfeature274   2.348e+03  3.554e+03   0.661 0.508825    \nfeature275  -3.518e+03  3.437e+03  -1.024 0.306149    \nfeature276   9.059e+02  3.231e+03   0.280 0.779242    \nfeature277   5.254e+03  2.862e+03   1.836 0.066550 .  \nfeature278   3.607e+03  3.234e+03   1.115 0.264834    \nfeature279  -6.503e+03  3.908e+03  -1.664 0.096272 .  \nfeature280   2.292e+03  2.957e+03   0.775 0.438402    \nfeature281  -4.644e+03  3.702e+03  -1.255 0.209744    \nfeature282  -2.065e+03  3.490e+03  -0.592 0.554038    \nfeature283   2.242e+03  3.732e+03   0.601 0.548041    \nfeature284   4.647e+03  3.343e+03   1.390 0.164673    \nfeature285   3.772e+02  3.212e+03   0.117 0.906533    \nfeature286   3.963e+03  3.169e+03   1.251 0.211194    \nfeature287  -2.063e+03  1.536e+03  -1.343 0.179493    \nfeature288  -6.668e+03  2.911e+03  -2.290 0.022104 *  \nfeature289  -1.099e+04  2.774e+03  -3.963 7.66e-05 ***\nfeature290  -5.420e+03  2.889e+03  -1.876 0.060838 .  \nfeature291   8.285e+03  3.339e+03   2.481 0.013182 *  \nfeature292   4.481e+03  4.070e+03   1.101 0.270970    \nfeature293  -5.847e+03  3.282e+03  -1.782 0.074932 .  \nfeature294  -4.980e+03  3.438e+03  -1.449 0.147595    \nfeature295   4.927e+03  3.097e+03   1.591 0.111744    \nfeature296  -8.711e+03  2.512e+03  -3.467 0.000537 ***\nfeature297  -5.638e+03  3.892e+03  -1.449 0.147603    \nfeature298   1.434e+03  3.150e+03   0.455 0.648981    \nfeature299  -5.802e+03  3.719e+03  -1.560 0.118883    \nfeature300  -5.712e+03  3.124e+03  -1.829 0.067613 .  \nfeature301  -2.137e+03  2.539e+03  -0.842 0.400011    \nfeature302   5.517e+03  2.826e+03   1.952 0.051058 .  \nfeature303  -1.182e+04  4.186e+03  -2.823 0.004805 ** \nfeature304   1.170e+04  3.105e+03   3.769 0.000169 ***\nfeature305   5.272e+03  3.143e+03   1.678 0.093569 .  \nfeature306   4.727e+03  4.769e+03   0.991 0.321739    \nfeature307  -7.177e+03  2.558e+03  -2.806 0.005070 ** \nfeature308   9.804e+03  3.746e+03   2.617 0.008936 ** \nfeature309   1.822e+04  3.847e+03   4.736 2.32e-06 ***\nfeature310   2.236e+02  2.331e+03   0.096 0.923611    \nfeature311   1.456e+02  3.988e+03   0.037 0.970887    \nfeature312  -5.665e+03  3.490e+03  -1.623 0.104743    \nfeature313  -2.280e+03  3.457e+03  -0.660 0.509605    \nfeature314  -1.620e+03  2.798e+03  -0.579 0.562686    \nfeature315  -7.906e+01  2.455e+03  -0.032 0.974313    \nfeature316  -6.722e+03  3.591e+03  -1.872 0.061374 .  \nfeature317  -9.344e+03  3.307e+03  -2.826 0.004760 ** \nfeature318  -9.038e+02  3.091e+03  -0.292 0.770022    \nfeature319  -1.135e+03  3.687e+03  -0.308 0.758185    \nfeature320  -1.835e+02  3.665e+03  -0.050 0.960071    \nfeature321  -8.836e+00  3.030e+03  -0.003 0.997673    \nfeature322   5.219e+03  2.947e+03   1.771 0.076683 .  \nfeature323  -6.288e+02  3.378e+03  -0.186 0.852329    \nfeature324   1.704e+03  3.356e+03   0.508 0.611776    \nfeature325  -5.609e+03  3.899e+03  -1.438 0.150457    \nfeature326   1.778e+03  1.073e+03   1.658 0.097559 .  \nfeature327  -2.830e+03  2.988e+03  -0.947 0.343671    \nfeature328  -5.539e+04  5.781e+04  -0.958 0.338057    \nfeature329  -3.916e+03  2.409e+03  -1.626 0.104145    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 62540 on 2100 degrees of freedom\nMultiple R-squared:  0.8829,    Adjusted R-squared:  0.8645 \nF-statistic:    48 on 330 and 2100 DF,  p-value: &lt; 2.2e-16\n\n\nOn training data, adjusted R-squared: 0.8645, which indicates the model explains a high proportion of variance in home values.\n\n\nCode\n# Make predictions on test set\ny_pred_lr &lt;- predict(lr_model, newdata = test_data%&gt;%dplyr::select(-index, -X2024.10.31))\n\ny_test &lt;- test_data$X2024.10.31\n\n# Calculate evaluation metrics\nlr_mae &lt;- mae(y_test, y_pred_lr)\nlr_rmse &lt;- rmse(y_test, y_pred_lr)\nlr_r2 &lt;- cor(y_test, y_pred_lr)^2\n# Display results\nlr_results &lt;- data.frame(\n  Metric = c(\"MAE\", \"RMSE\", \"R²\"),\n  Value = c(\n    round(lr_mae, 2),\n    round(lr_rmse, 2),\n    round(lr_r2, 4)\n  )\n)\nkable(lr_results,\n      caption = \"Linear Regression Performance Metrics\",\n      col.names = c(\"Metric\", \"Value\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nLinear Regression Performance Metrics\n\n\nMetric\nValue\n\n\n\n\nMAE\n42972.1200\n\n\nRMSE\n59355.7100\n\n\nR²\n0.8565\n\n\n\n\n\nLinear Regression Interpretation:\n\nUses all 330 embedding features\nNo feature selection - may include irrelevant features\nServes as baseline for comparison\n\n\n\nModel 2: Stepwise Regression\n\n\nCode\n# Perform stepwise regression using AIC criterion\n# Direction \"both\" allows both forward and backward selection\n# step_model &lt;- stepAIC(\n#   lr_model,\n#   direction = \"both\",\n#   trace = TRUE  # Set to TRUE to see step-by-step selection\n# )\n\n\nStepwise Regression Interpretation:\n\nAutomatically selects most predictive features using AIC (Akaike Information Criterion)\nBalances model fit with complexity\nTypically results in a more parsimonious model\nShows which embedding dimensions are most important for prediction\nNot possible (it will compute over 54000 models, roughly takes around 6-7 days to finish)\n\n\n\nModel 3: Ridge Regression\n\n\nCode\n# Prepare matrix format for glmnet (Ridge regression requires matrix input)\nx_train &lt;- as.matrix(train_data %&gt;% dplyr::select(-target))\ny_train &lt;- train_data$target\nx_test &lt;- as.matrix(test_data %&gt;% dplyr::select(-index, -X2024.10.31))\n\n# Perform cross-validation to find optimal lambda\ncv_ridge &lt;- cv.glmnet(x_train, y_train, alpha = 0, nfolds = 10)\n\n# Fit Ridge regression model with optimal lambda\nridge_model &lt;- glmnet(x_train, y_train, alpha = 0, lambda = cv_ridge$lambda.min)\n\n# Make predictions on test set\ny_pred_ridge &lt;- predict(ridge_model, newx = x_test, s = cv_ridge$lambda.min)\n\n# Calculate evaluation metrics\nridge_mae &lt;- mae(y_test, y_pred_ridge)\nridge_rmse &lt;- rmse(y_test, y_pred_ridge)\nridge_r2 &lt;- cor(y_test, y_pred_ridge)^2\n\n# Display results\nridge_results &lt;- data.frame(\n  Metric = c(\"MAE\", \"RMSE\", \"R²\", \"Optimal Lambda\"),\n  Value = c(\n    round(ridge_mae, 2),\n    round(ridge_rmse, 2),\n    round(ridge_r2, 4),\n    round(cv_ridge$lambda.min, 4)\n  )\n)\n\nkable(ridge_results,\n      caption = \"Ridge Regression Performance Metrics\",\n      col.names = c(\"Metric\", \"Value\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nRidge Regression Performance Metrics\n\n\nMetric\nValue\n\n\n\n\nMAE\n40895.1000\n\n\nRMSE\n57471.6900\n\n\nR²\n0.8606\n\n\nOptimal Lambda\n14210.2992\n\n\n\n\n\nRidge Regression Interpretation:\n\nUses L2 regularization to penalize large coefficients\nShrinks coefficients but keeps all 330 features (does not perform feature selection)\nLambda parameter controls the amount of regularization\nCross-validation used to find optimal lambda value\nHelps prevent overfitting compared to standard linear regression\n\n\n\nModel 4: Lasso Regression\n\n\nCode\n# Perform cross-validation to find optimal lambda for Lasso\ncv_lasso &lt;- cv.glmnet(x_train, y_train, alpha = 1, nfolds = 10)\n\n# Fit Lasso regression model with optimal lambda\nlasso_model &lt;- glmnet(x_train, y_train, alpha = 1, lambda = cv_lasso$lambda.min)\n\n# Make predictions on test set\ny_pred_lasso &lt;- predict(lasso_model, newx = x_test, s = cv_lasso$lambda.min)\n\n# Calculate evaluation metrics\nlasso_mae &lt;- mae(y_test, y_pred_lasso)\nlasso_rmse &lt;- rmse(y_test, y_pred_lasso)\nlasso_r2 &lt;- cor(y_test, y_pred_lasso)^2\n\n# Count number of non-zero coefficients (features selected)\nlasso_coefs &lt;- coef(lasso_model, s = cv_lasso$lambda.min)\nn_features_selected &lt;- sum(lasso_coefs != 0) - 1  # Subtract 1 for intercept\n\n# Display results\nlasso_results &lt;- data.frame(\n  Metric = c(\"MAE\", \"RMSE\", \"R²\", \"Optimal Lambda\", \"Features Selected\"),\n  Value = c(\n    round(lasso_mae, 2),\n    round(lasso_rmse, 2),\n    round(lasso_r2, 4),\n    round(cv_lasso$lambda.min, 4),\n    n_features_selected\n  )\n)\n\nkable(lasso_results,\n      caption = \"Lasso Regression Performance Metrics\",\n      col.names = c(\"Metric\", \"Value\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nLasso Regression Performance Metrics\n\n\nMetric\nValue\n\n\n\n\nMAE\n41944.9500\n\n\nRMSE\n58234.7900\n\n\nR²\n0.8581\n\n\nOptimal Lambda\n587.1713\n\n\nFeatures Selected\n229.0000\n\n\n\n\n\nLasso Regression Interpretation:\n\nUses L1 regularization to penalize large coefficients\nPerforms automatic feature selection by shrinking some coefficients to exactly zero\nLambda parameter controls the amount of regularization\nCross-validation used to find optimal lambda value\nResults in a sparse model with fewer features than Ridge regression\nHelps identify the most important embedding dimensions for prediction\n\n\n\nModel Comparison\n\n\nCode\n# Create comparison dataframe with all models\nmodel_comparison &lt;- data.frame(\n  Model = c(\"Linear Regression\", \"Ridge Regression\", \"Lasso Regression\"),\n  MAE = c(\n    round(lr_mae, 2),\n    round(ridge_mae, 2),\n    round(lasso_mae, 2)\n  ),\n  RMSE = c(\n    round(lr_rmse, 2),\n    round(ridge_rmse, 2),\n    round(lasso_rmse, 2)\n  ),\n  R_squared = c(\n    round(lr_r2, 4),\n    round(ridge_r2, 4),\n    round(lasso_r2, 4)\n  ),\n  Features_Used = c(\n    330,\n    330,\n    n_features_selected\n  )\n)\n\nkable(model_comparison,\n      caption = \"Comparison of Regression Models\",\n      col.names = c(\"Model\", \"MAE\", \"RMSE\", \"R²\", \"Features Used\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\")) %&gt;%\n  row_spec(which.min(model_comparison$RMSE), bold = TRUE, color = \"white\", background = \"#4CAF50\")\n\n\n\nComparison of Regression Models\n\n\nModel\nMAE\nRMSE\nR²\nFeatures Used\n\n\n\n\nLinear Regression\n42972.12\n59355.71\n0.8565\n330\n\n\nRidge Regression\n40895.10\n57471.69\n0.8606\n330\n\n\nLasso Regression\n41944.95\n58234.79\n0.8581\n229\n\n\n\n\n\nKey Insights:\n\nCompare which model performs better on unseen data\nRidge regression uses all features but with regularization to prevent overfitting\nLasso regression performs feature selection, using fewer features while maintaining accuracy\nFeature reduction can lead to better interpretability and faster predictions\nThe best performing model (lowest RMSE) is highlighted in green"
  },
  {
    "objectID": "pdfm/demo1/demo1.html#visualization-of-model-performance",
    "href": "pdfm/demo1/demo1.html#visualization-of-model-performance",
    "title": "Zillow Home Value Index Analysis with PDFM Embeddings",
    "section": "Visualization of Model Performance",
    "text": "Visualization of Model Performance\n\nActual vs. Predicted Plot - Linear Regression\n\n\nCode\n# Create evaluation dataframe for linear regression\neval_df_lr &lt;- data.frame(\n  actual = y_test,\n  predicted = y_pred_lr\n)\n\n# Plot\nggplot(eval_df_lr, aes(x = actual, y = predicted)) +\n  geom_point(alpha = 0.5, color = \"steelblue\") +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  coord_fixed(xlim = c(0, 1000000), ylim = c(0, 1000000)) +\n  labs(\n    title = \"Linear Regression: Actual vs Predicted\",\n    subtitle = paste0(\"R² = \", round(lr_r2, 4)),\n    x = \"Actual Home Value ($)\",\n    y = \"Predicted Home Value ($)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5)\n  ) +\n  scale_x_continuous(labels = scales::comma) +\n  scale_y_continuous(labels = scales::comma)\n\n\n\n\n\n\n\n\n\n\n\nActual vs. Predicted Plot - Ridge Regression\n\n\nCode\n# Create evaluation dataframe for Ridge regression\neval_df_ridge &lt;- data.frame(\n  actual = y_test,\n  predicted = as.vector(y_pred_ridge)\n)\n\n# Plot\nggplot(eval_df_ridge, aes(x = actual, y = predicted)) +\n  geom_point(alpha = 0.5, color = \"darkgreen\") +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  coord_fixed(xlim = c(0, 1000000), ylim = c(0, 1000000)) +\n  labs(\n    title = \"Ridge Regression: Actual vs Predicted\",\n    subtitle = paste0(\"R² = \", round(ridge_r2, 4)),\n    x = \"Actual Home Value ($)\",\n    y = \"Predicted Home Value ($)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5)\n  ) +\n  scale_x_continuous(labels = scales::comma) +\n  scale_y_continuous(labels = scales::comma)\n\n\n\n\n\n\n\n\n\nInterpretation of Scatter Plots:\n\nPoints along the red diagonal line indicate perfect predictions\nPoints above the line = model overestimates home value\nPoints below the line = model underestimates home value\nTighter clustering around the diagonal = better model performance\n\n\n\nActual vs. Predicted Plot - Lasso Regression\n\n\nCode\n# Create evaluation dataframe for Lasso regression\neval_df_lasso &lt;- data.frame(\n  actual = y_test,\n  predicted = as.vector(y_pred_lasso)\n)\n\n# Plot\nggplot(eval_df_lasso, aes(x = actual, y = predicted)) +\n  geom_point(alpha = 0.5, color = \"darkorange\") +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  coord_fixed(xlim = c(0, 1000000), ylim = c(0, 1000000)) +\n  labs(\n    title = \"Lasso Regression: Actual vs Predicted\",\n    subtitle = paste0(\"R² = \", round(lasso_r2, 4), \" | Features: \", n_features_selected, \"/330\"),\n    x = \"Actual Home Value ($)\",\n    y = \"Predicted Home Value ($)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5)\n  ) +\n  scale_x_continuous(labels = scales::comma) +\n  scale_y_continuous(labels = scales::comma)"
  },
  {
    "objectID": "pdfm/demo1/demo1.html#spatial-visualization-of-prediction-errors",
    "href": "pdfm/demo1/demo1.html#spatial-visualization-of-prediction-errors",
    "title": "Zillow Home Value Index Analysis with PDFM Embeddings",
    "section": "Spatial Visualization of Prediction Errors",
    "text": "Spatial Visualization of Prediction Errors\n\nCalculate Prediction Differences\n\n\nCode\n# Add test data indices back to identify counties\ntest_data_with_place &lt;- modeling_data[-train_indices, ] %&gt;%\n  mutate(row_idx = row_number())\n\n# Get the original place identifiers for test data\ntest_places &lt;- data[test_data$index, \"place\"]\n\n# Create comparison dataframe\nprediction_comparison &lt;- data.frame(\n  place = test_places,\n  actual = y_test,\n  pred_lr = y_pred_lr,\n  pred_ridge = as.vector(y_pred_ridge),\n  pred_lasso = as.vector(y_pred_lasso)\n) %&gt;%\n  mutate(\n    diff_lr = pred_lr - actual,\n    diff_ridge = pred_ridge - actual,\n    diff_lasso = pred_lasso - actual\n  )\n\n\n\n\nMap Prediction Errors\nLinear Regression Errors:\n\n\nCode\n# Join prediction differences with county geometries\nerror_map_data_lr &lt;- county_gdf %&gt;%\n  inner_join(prediction_comparison, by = \"place\")\n# Create color palette for differences (red = underestimate, blue = overestimate)\npal_diff_lr &lt;- colorNumeric(\n  palette = c(\"blue\", \"white\", \"red\"),\n  domain = c(-200000, 200000),\n  na.color = \"gray\"\n)\n# Create interactive map showing Linear regression errors\nleaflet(error_map_data_lr) %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    fillColor = ~pal_diff_lr(diff_lr),\n    fillOpacity = 0.7,\n    color = \"white\",\n    weight = 1,\n    popup = ~paste0(\n      \"&lt;strong&gt;County&lt;/strong&gt;&lt;br&gt;\",\n      \"Actual: $\", format(round(actual), big.mark = \",\"), \"&lt;br&gt;\",\n      \"Predicted (Linear): $\", format(round(pred_lr), big.mark = \",\"), \"&lt;br&gt;\",\n      \"Difference: $\", format(round(diff_lr), big.mark = \",\")\n    )\n  ) %&gt;%\n  addLegend(\n    position = \"bottomright\",\n    pal = pal_diff_lr,\n    values = ~diff_lr,\n    title = \"Prediction Error&lt;br&gt;(Linear Model)\",\n    opacity = 1,\n    labFormat = labelFormat(prefix = \"$\")\n  )\n\n\n\n\n\n\nRidge Regression Errors:\n\n\nCode\n# Join prediction differences with county geometries\nerror_map_data_ridge &lt;- county_gdf %&gt;%\n  inner_join(prediction_comparison, by = \"place\")\n# Create color palette for differences (red = underestimate, blue = overestimate)\npal_diff_ridge &lt;- colorNumeric(\n  palette = c(\"blue\", \"white\", \"red\"),\n  domain = c(-200000, 200000),\n  na.color = \"gray\"\n)\n# Create interactive map showing Ridge regression errors\nleaflet(error_map_data_ridge) %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    fillColor = ~pal_diff_ridge(diff_ridge),\n    fillOpacity = 0.7,\n    color = \"white\",\n    weight = 1,\n    popup = ~paste0(\n      \"&lt;strong&gt;County&lt;/strong&gt;&lt;br&gt;\",\n      \"Actual: $\", format(round(actual), big.mark = \",\"), \"&lt;br&gt;\",\n      \"Predicted (Ridge): $\", format(round(pred_ridge), big.mark = \",\"), \"&lt;br&gt;\",\n      \"Difference: $\", format(round(diff_ridge), big.mark = \",\")\n    )\n  ) %&gt;%\n  addLegend(\n    position = \"bottomright\",\n    pal = pal_diff_ridge,\n    values = ~diff_ridge,\n    title = \"Prediction Error&lt;br&gt;(Ridge Model)\",\n    opacity = 1,\n    labFormat = labelFormat(prefix = \"$\")\n  )\n\n\n\n\n\n\nLasso Regression Errors:\n\n\nCode\n# Join prediction differences with county geometries\nerror_map_data &lt;- county_gdf %&gt;%\n  inner_join(prediction_comparison, by = \"place\")\n\n# Create color palette for differences (red = underestimate, blue = overestimate)\npal_diff &lt;- colorNumeric(\n  palette = c(\"blue\", \"white\", \"red\"),\n  domain = c(-200000, 200000),\n  na.color = \"gray\"\n)\n\n# Create interactive map showing Lasso regression errors\nleaflet(error_map_data) %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    fillColor = ~pal_diff(diff_lasso),\n    fillOpacity = 0.7,\n    color = \"white\",\n    weight = 1,\n    popup = ~paste0(\n      \"&lt;strong&gt;County&lt;/strong&gt;&lt;br&gt;\",\n      \"Actual: $\", format(round(actual), big.mark = \",\"), \"&lt;br&gt;\",\n      \"Predicted (Lasso): $\", format(round(pred_lasso), big.mark = \",\"), \"&lt;br&gt;\",\n      \"Difference: $\", format(round(diff_lasso), big.mark = \",\")\n    )\n  ) %&gt;%\n  addLegend(\n    position = \"bottomright\",\n    pal = pal_diff,\n    values = ~diff_lasso,\n    title = \"Prediction Error&lt;br&gt;(Lasso Model)\",\n    opacity = 1,\n    labFormat = labelFormat(prefix = \"$\")\n  )\n\n\n\n\n\n\nSpatial Error Analysis:\n\nRed areas: Model underestimates home values (actual &gt; predicted)\nBlue areas: Model overestimates home values (actual &lt; predicted)\nWhite areas: Predictions close to actual values\nThis spatial visualization can reveal geographic patterns in model performance\nSystematic errors in specific regions may indicate missing spatial features or local market conditions not captured by embeddings"
  },
  {
    "objectID": "pdfm/demo1/demo1.html#summary-and-conclusions",
    "href": "pdfm/demo1/demo1.html#summary-and-conclusions",
    "title": "Zillow Home Value Index Analysis with PDFM Embeddings",
    "section": "Summary and Conclusions",
    "text": "Summary and Conclusions\n\nKey Findings\n\nPDFM Embeddings as Features: The 330-dimensional PDFM embeddings successfully capture spatial patterns relevant to home value prediction.\nModel Performance:\n\nLinear regression provides a straightforward baseline using all features\nRidge regression uses L2 regularization to reduce overfitting while keeping all features\nLasso regression performs automatic feature selection through L1 regularization\n\nRegularization Benefits:\n\nReduced overfitting compared to standard linear regression\nRidge: Shrinks coefficients but maintains all 330 features\nLasso: Identifies most important embedding dimensions through feature selection\nCross-validation ensures optimal regularization strength (lambda)\nImproved generalization to unseen data\n\nSpatial Patterns: Error maps reveal geographic variations in prediction accuracy, suggesting opportunities for:\n\nRegional model calibration\nIncorporation of additional local features\nInvestigation of market-specific factors\n\n\n\n\nMethodological Considerations\nAdvantages of Using Embeddings:\n\nCaptures complex, non-linear relationships\nIncorporates diverse data sources (mobility, search trends, environment)\nTransfer learning from large-scale models\nRich spatial representation\n\nLimitations:\n\nBlack-box nature makes interpretation challenging\nEmbeddings may encode biases from training data\n\n\n\nFuture Directions\n\nAdvanced Models: Try ensemble methods (Random Forest, XGBoost) or neural networks\nFeature Engineering: Combine embeddings with traditional features (square footage, bedrooms, etc.)"
  },
  {
    "objectID": "pdfm/demo1/demo1.html#references",
    "href": "pdfm/demo1/demo1.html#references",
    "title": "Zillow Home Value Index Analysis with PDFM Embeddings",
    "section": "References",
    "text": "References\n\nGoogle Research: Population Dynamics Foundation Model (PDFM)\nZillow Home Value Index (ZHVI) Database\nDr. Qiusheng Wu PDFM Tutorial\nGoogle PDFM Embedding"
  },
  {
    "objectID": "pdfm/zillow_pdfm_visu.html",
    "href": "pdfm/zillow_pdfm_visu.html",
    "title": "Google Embeddings tutorial",
    "section": "",
    "text": "Code\nimport os\nimport pandas as pd\nimport geopandas as gpd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom leafmap.common import evaluate_model, plot_actual_vs_predicted, download_file\nimport leafmap.maplibregl as leafmap\n\n\n\n\nCode\nzhvi = \"https://github.com/opengeos/datasets/releases/download/us/zillow_home_value_index_by_county.csv\"\nzhvo_file = \"zillow_home_value_index_by_county.csv\"\nif not os.path.exists(zhvo_file):\n    download_file(zhvi, zhvo_file)\n\n\n\n\nCode\nzhvi_df = pd.read_csv(zhvo_file, dtype={\"StateCodeFIPS\": str, \"MunicipalCodeFIPS\": str})\nzhvi_df.index = \"geoId/\" + zhvi_df[\"StateCodeFIPS\"] + zhvi_df[\"MunicipalCodeFIPS\"]\nzhvi_df.head()\n\n\n\n\nCode\ncounty_geojson = \"/home/zyang91/Desktop/us/county.geojson\"\n\n\n\n\nCode\ncounty_gdf = gpd.read_file(county_geojson)\ncounty_gdf.set_index(\"place\", inplace=True)\ncounty_gdf.head()\n\n\n\n\nCode\ndf = zhvi_df.join(county_gdf)\ndf\n\n\n\n\nCode\nzhvi_gdf = gpd.GeoDataFrame(df, geometry=\"geometry\")\nzhvi_gdf.head()\n\n\n\n\nCode\ncolumn = \"2024-10-31\"\ngdf = zhvi_gdf[[\"RegionName\", \"State\", column, \"geometry\"]]\ngdf.head()\n\n\n\n\nCode\nm = leafmap.Map(style=\"liberty\")\nm.add_data(\n    gdf,\n    cmap=\"Blues\",\n    column=column,\n    legend_title=\"Zillow Home Median Home Value\",\n    name=\"Zillow Home Median Home Value\",\n)\nm.add_layer_control()\nm\n\n\n\n\nCode\nm = leafmap.Map(style=\"liberty\", pitch=60)\nm.add_data(\n    gdf,\n    cmap=\"Blues\",\n    column=column,\n    extrude=True,\n    scale_factor=3,\n    legend_title=\"Zillow Home Median Home Value\",\n    name=\"Zillow Home Median Home Value\",\n)\nm.add_layer_control()\nm\n\n\n\n\nCode\nembeddings = pd.read_csv(\"/home/zyang91/Desktop/us/county_embeddings.csv\").set_index(\n    \"place\"\n)\nembeddings.head()\n\n\n\n\nCode\ndf = embeddings.join(county_gdf)\n\n\n\n\nCode\nembeddings_gdf = gpd.GeoDataFrame(df, geometry=\"geometry\")\nembeddings_gdf.head()\n\n\n\n\nCode\ncolumn = \"feature329\"\ngdf = embeddings_gdf[[\"state\", column, \"geometry\"]]\ngdf.head()\n\n\n\n\nCode\nm = leafmap.Map(style=\"liberty\")\nm.add_data(\n    gdf,\n    cmap=\"Blues\",\n    column=column,\n    legend_title=column,\n    name=column,\n)\nm.add_layer_control()\nm\n\n\n\n\nCode\nm = leafmap.Map(style=\"liberty\", pitch=60)\nm.add_data(\n    gdf,\n    cmap=\"Blues\",\n    column=column,\n    extrude=True,\n    scale_factor=0.00005,\n    legend_title=column,\n    name=column,\n)\nm.add_layer_control()\nm\n\n\n\n\nCode\ndata = zhvi_df.join(embeddings, how=\"inner\")\ndata.head()\n\n\n\n\nCode\nembedding_features = [f\"feature{i}\" for i in range(330)]\nlabel = \"2024-10-31\"\n\n\n\n\nCode\ndata = data.dropna(subset=[label])\n\n\n\n\nCode\ndata = data[embedding_features + [label]]\nx = data[embedding_features]\ny = data[label]\nx_train, x_test, y_train, y_test = train_test_split(\n    x, y, test_size=0.2, random_state=42\n)\n\n\n\n\nCode\nmodel = LinearRegression()\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\n\n\n\n\nCode\nevaluation_df = pd.DataFrame({\"y\": y_test, \"y_pred\": y_pred})\nmetrics = evaluate_model(evaluation_df)\nprint(metrics)\n\n\n\n\nCode\nxy_lim = (0, 1000000)\nplot_actual_vs_predicted(\n    evaluation_df,\n    xlim=xy_lim,\n    ylim=xy_lim,\n    title=\"Linear Regression: Actual vs Predicted\",\n    x_label=\"Actual Home Value\",\n    y_label=\"Predicted Home Value\",\n)\n\n\n\n\nCode\ndf = evaluation_df.join(gdf)\ndf[\"difference\"] = df[\"y\"] - df[\"y_pred\"]\n\n\n\n\nCode\ndf.head()\n\n\n\n\nCode\ngdf = gpd.GeoDataFrame(df, geometry=\"geometry\")\ngdf.head()\n\n\n\n\nCode\ngdf.drop(columns=[\"category\", \"color\", column], inplace=True)\ngdf.head()\n\n\n\n\nCode\nm = leafmap.Map(style=\"liberty\")\nm.add_data(\n    gdf,\n    cmap=\"Blues\",\n    column=\"difference\",\n    legend_title=\"Difference (Actual - Predicted)\",\n    name=\"Difference\",\n)\nm.add_layer_control()\nm"
  },
  {
    "objectID": "pdfm/zillow_pdfm.html",
    "href": "pdfm/zillow_pdfm.html",
    "title": "Google Embeddings tutorial",
    "section": "",
    "text": "Code\nimport os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom leafmap.common import evaluate_model, plot_actual_vs_predicted, download_file\n\n\n\n\nCode\nzhvi_file = \"/home/zyang91/Desktop/data/zillow_home_value_index_by_zipcode.csv\"\n\n\n\n\nCode\nzhvi_df = pd.read_csv(zhvi_file, dtype={\"RegionName\": str})\nzhvi_df.index = zhvi_df[\"RegionName\"].apply(lambda x: f\"zip/{x}\")\nzhvi_df\n\n\n\n\nCode\nembeddings_file = \"/home/zyang91/Desktop/us/zcta_embeddings.csv\"\n\n\n\n\nCode\nzipcode_embeddings = pd.read_csv(embeddings_file).set_index(\"place\")\nzipcode_embeddings\n\n\n\n\nCode\ndata = zhvi_df.join(zipcode_embeddings, how=\"inner\")\ndata\n\n\n\n\nCode\nembedding_features = [f\"feature{x}\" for x in range(330)]\nlabel = \"2024-10-31\"\n\n\n\n\nCode\ndata = data.dropna(subset=[label])\ndata\n\n\n\n\nCode\ndata = data[embedding_features + [label]]\nX = data[embedding_features]\ny = data[label]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n\n\n\nCode\n# Initialize and train a simple linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred = model.predict(X_test)\n\n\n\n\nCode\nevaluation_df = pd.DataFrame({\"y\": y_test, \"y_pred\": y_pred})\nmetrics = evaluate_model(evaluation_df)\nprint(metrics)\n\n\n\n\nCode\nxy_lim = (0, 3_000_000)\nplot_actual_vs_predicted(\n    evaluation_df,\n    xlim=xy_lim,\n    ylim=xy_lim,\n    title=\"Actual vs Predicted Home Values\",\n    x_label=\"Actual Home Value\",\n    y_label=\"Predicted Home Value\",\n)\n\n\n\n\nCode\nevaluate_df = pd.DataFrame({\"y\": y_test, \"y_pred\": y_pred})\nmetrics = evaluate_model(evaluate_df)\nprint(metrics)\n\n\n\n\nCode\nevaluate_df.head()\n\n\n\n\nCode\nxy_lim = (0, 3000000)\nplot_actual_vs_predicted(\n    evaluate_df,\n    x_label=\"Actual\",\n    y_label=\"Predicted\",\n    xlim=xy_lim,\n    ylim=xy_lim,\n    title=\"Actual vs Predicted ZHVI\",\n)\n\n\n\n\nCode\nk = 5\nknn_model = KNeighborsRegressor(n_neighbors=k)\nknn_model.fit(X_train, y_train)\ny_pred = knn_model.predict(X_test)\n\n\n\n\nCode\nplot_actual_vs_predicted(\n    evaluate_df,\n    x_label=\"Actual\",\n    y_label=\"Predicted\",\n    xlim=xy_lim,\n    ylim=xy_lim,\n    title=f\"Actual vs Predicted ZHVI (KNN, k={k})\",\n)\n\n\n\n\nCode\nevaluation_df = pd.DataFrame({\"y\": y_test, \"y_pred\": y_pred})\nmetrics = evaluate_model(evaluation_df)\nprint(metrics)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Google Embeddings tutorial",
    "section": "",
    "text": "Author: Zhanchao Yang"
  },
  {
    "objectID": "index.html#gis-day-lightning-talk-for-the-university-of-pennsylvania",
    "href": "index.html#gis-day-lightning-talk-for-the-university-of-pennsylvania",
    "title": "Google Embeddings tutorial",
    "section": "GIS day lightning talk for the University of Pennsylvania",
    "text": "GIS day lightning talk for the University of Pennsylvania\n11.19.2025\n\nTopic\nSatellite and Population Embeddings: Powerful GeoAI Tools or Overrated? ### Abstract\nIn recent years, Google has introduced two powerful embedding for spatial analysis: satellite imagery embeddings, which capture visual and environmental patterns from overhead imagery, and population embeddings, which encode demographic and mobility information at high spatial resolutions. These embeddings promise to make geospatial machine learning more scalable, flexible, and transferable, but do they deliver? This talk will begin with an introduction to both embedding types, representation, and the types of spatial signals they capture. A short live demo will walk through two practical use cases: land use classification and estimating housing prices using these embeddings as inputs. These examples will illustrate the potential of embedding-based GeoAI to outperform traditional feature engineering. However, the talk will also highlight critical limitations: the opacity of learned representations, challenges in interpretability and validation, and the risk of reinforcing spatial and social biases. By the end, attendees will gain a clear understanding of both the capabilities and caveats of these embeddings, and when (or whether) they should be trusted in decision-making contexts."
  },
  {
    "objectID": "index.html#type-of-embedding",
    "href": "index.html#type-of-embedding",
    "title": "Google Embeddings tutorial",
    "section": "Type of Embedding",
    "text": "Type of Embedding\n\nPDFM Embedding\nGoogle Earth Embedding\n\nCredit:\n\nDr. Qiusheng Wu PDFM tutorial\nDr. Qiusheng Wu Google Earth Embedding tutorial\nGoogle official JavaScript Satellite Embedding tutorial\nGoogle PDFM Embedding\nGoogle Earth Satellite Embedding\n\nALL Exert or modify from original Repo, all rights reserved to the original author!"
  },
  {
    "objectID": "index.html#population-dynamics-foundation-model-pdfm-embeddings",
    "href": "index.html#population-dynamics-foundation-model-pdfm-embeddings",
    "title": "Google Embeddings tutorial",
    "section": "Population Dynamics Foundation Model (PDFM) Embeddings",
    "text": "Population Dynamics Foundation Model (PDFM) Embeddings\nPDFM Embeddings are condensed vector representations designed to encapsulate the complex, multidimensional interactions among human behaviors, environmental factors, and local contexts at specific locations. These embeddings capture patterns in aggregated data such as search trends, busyness trends, and environmental conditions (maps, air quality, temperature), providing a rich, location-specific snapshot of how populations engage with their surroundings. Aggregated over space and time, these embeddings ensure privacy while enabling nuanced spatial analysis and prediction for applications ranging from public health to socioeconomic modeling."
  },
  {
    "objectID": "index.html#application",
    "href": "index.html#application",
    "title": "Google Embeddings tutorial",
    "section": "Application",
    "text": "Application\nPDFM Embeddings can be applied to a wide range of geospatial prediction tasks, similar to census and socioeconomic statistics. Example use cases include:\n\nPopulation Health Outcomes: Predicting health statistics like disease prevalence or population health risks.\nSocioeconomic Factors: Modeling economic indicators and living conditions.\nRetail: Identifying promising locations for new stores, market expansion, and demand forecasting.\nMarketing and Sales: Characterizing high-performance regions and identifying similar areas to optimize marketing and sales efforts.\n\nBy incorporating spatial relationships and diverse feature types, these embeddings serve as a powerful tool for geospatial predictions."
  },
  {
    "objectID": "satellite-embedding/google_earth_embedding.html",
    "href": "satellite-embedding/google_earth_embedding.html",
    "title": "Google Embeddings tutorial",
    "section": "",
    "text": "Code\nimport ee\n\n\n\n\nCode\nimport leafmap.maplibregl as leafmap\n\n\n\n\nCode\nee.Authenticate()\n\n\n\n\nCode\nee.Initialize(project=\"ee-zhanchaoyang\")\n\n\n\n\nCode\nm = leafmap.Map(projection=\"globe\", sidebar_visible=True)\nm.add_basemap(\"USGS.Imagery\")\nm.add_alphaearth_gui()\nm\n\n\n\n\nCode\nm = leafmap.Map(projection=\"globe\", sidebar_visible=True)\nm.add_basemap(\"USGS.Imagery\")\nm\n\n\n\n\nCode\nlon = -75.191\nlat = 39.9515\nm.set_center(lon, lat, zoom=16)\n\n\n\n\nCode\npoint = ee.Geometry.Point(lon, lat)\ndataset = ee.ImageCollection(\"GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL\")\n\n\n\n\nCode\nimage1 = dataset.filterDate(\"2017-01-01\", \"2018-01-01\").filterBounds(point).first()\nimage2 = dataset.filterDate(\"2024-01-01\", \"2025-01-01\").filterBounds(point).first()\n\n\n\n\nCode\nvis_params = {\"min\": -0.3, \"max\": 0.3, \"bands\": [\"A01\", \"A16\", \"A09\"]}\nm.add_ee_layer(image1, vis_params, name=\"Year 1 embeddings\")\nm.add_ee_layer(image2, vis_params, name=\"Year 2 embeddings\")\n\n\n\n\nCode\ndot_prod = image1.multiply(image2).reduce(ee.Reducer.sum())\n\n\n\n\nCode\nvis_params = {\"min\": 0, \"max\": 1, \"palette\": [\"white\", \"black\"]}\nm.add_ee_layer(dot_prod, vis_params, name=\"Similarity\")\nm"
  },
  {
    "objectID": "satellite-embedding/regression.html",
    "href": "satellite-embedding/regression.html",
    "title": "Visulization",
    "section": "",
    "text": "Code\nimport geemap\nimport ee\nCode\nee.Authenticate()\nCode\nee.Initialize(project=\"ee-zhanchaoyang\")\nCode\nm = geemap.Map(center=(40, -100), zoom=4)\nm\nCode\ngeometry = ee.Geometry.Polygon(\n    [[74.322, 14.981], [74.322, 14.765], [74.648, 14.765], [74.648, 14.980]]\n)\nCode\nm.add_basemap(\"SATELLITE\")\nCode\nm.addLayer(geometry, {}, \"geometry\")\nm.centerObject(geometry)\nCode\nstartDate = ee.Date.fromYMD(2022, 1, 1)\nCode\nendDate = ee.Date.fromYMD(2023, 1, 1)\nCode\nembeddings = ee.ImageCollection(\"GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL\")\nCode\nembeddingsFiltered = embeddings.filter(ee.Filter.date(startDate, endDate)).filter(\n    ee.Filter.bounds(geometry)\n)\nCode\nembeddings_vis = embeddingsFiltered.median().clip(geometry)\nCode\nvis_params = {\n    \"bands\": [\"A00\", \"A01\", \"A02\"],\n    \"min\": -0.5,\n    \"max\": 0.5,\n}\nCode\nm.addLayer(embeddings_vis, vis_params, \"embeddings\")\nm"
  },
  {
    "objectID": "satellite-embedding/regression.html#tasks-continue",
    "href": "satellite-embedding/regression.html#tasks-continue",
    "title": "Visulization",
    "section": "Tasks continue",
    "text": "Tasks continue\n\n\nCode\n# check the projections\nembeddingsProjection = ee.Image(embeddingsFiltered.first()).select(0).projection()\n\n\n\n\nCode\nembeddingsProjection\n\n\n\n\nCode\nembeddingsImage = embeddingsFiltered.mosaic().setDefaultProjection(embeddingsProjection)\n\n\n\n\nCode\ngedi = ee.ImageCollection(\"LARSE/GEDI/GEDI04_A_002_MONTHLY\")\n\n\n\n\nCode\ndef qualityMask(image):\n    mask = image.updateMask(image.select(\"l4_quality_flag\").eq(1)).updateMask(\n        image.select(\"degrade_flag\").eq(0)\n    )\n    return mask\n\n\n\n\nCode\ndef errorMask(image):\n    relative_se = image.select(\"agbd_se\").divide(image.select(\"agbd\"))\n    error_mask = image.updateMask(relative_se.lte(0.5))\n    return error_mask\n\n\n\n\nCode\ndef SlopeMask(image):\n    glo30 = ee.ImageCollection(\"COPERNICUS/DEM/GLO30\")\n    glo30Filtered = glo30.filter(ee.Filter.bounds(geometry)).select(\"DEM\")\n    demProj = glo30Filtered.first().select(0).projection()\n    elevation = glo30Filtered.mosaic().rename(\"dem\").setDefaultProjection(demProj)\n    slope = ee.Terrain.slope(elevation)\n    slope_mask = image.updateMask(slope.lte(30))\n    return slope_mask\n\n\n\n\nCode\ngediFiltered = gedi.filter(ee.Filter.date(startDate, endDate)).filter(\n    ee.Filter.bounds(geometry)\n)\n\n\n\n\nCode\ngediProjection = ee.Image(gediFiltered.first()).select(\"agbd\").projection()\n\n\n\n\nCode\ngedi_processed = gediFiltered.map(qualityMask).map(errorMask).map(SlopeMask)\n\n\n\n\nCode\ngediMosaic = gedi_processed.mosaic().select(\"agbd\").setDefaultProjection(gediProjection)\n\n\n\n\nCode\ngediVis = {\n    min: 0,\n    max: 200,\n    \"palette\": [\"#edf8fb\", \"#b2e2e2\", \"#66c2a4\", \"#2ca25f\", \"#006d2c\"],\n    \"bands\": [\"agbd\"],\n}\n\n\n\n\nCode\nm.addLayer(gedi_processed, gediVis, \"gedi\")\nm\n\n\n\n\nCode\ngridScale = 100\n\n\n\n\nCode\ngridProjection = ee.Projection(\"EPSG:3857\").atScale(gridScale)\n\n\n\n\nCode\nstacked = embeddingsImage.addBands(gediMosaic)\n\n\n\n\nCode\nstacked = stacked.resample(\"bilinear\")\n\n\n\n\nCode\nstackedResampled = stacked.reduceResolution(\n    reducer=ee.Reducer.mean(), maxPixels=1024\n).reproject(crs=gridProjection)\n\n\n\n\nCode\nstackedResampled = stackedResampled.updateMask(stackedResampled.mask().gt(0))\n\n\n\n\nCode\nexport_image = stackedResampled.clip(geometry)\n\n\n\n\nCode\ngeemap.ee_export_image_to_drive(\n    export_image,\n    description=\"GEDI_Mosaic_Export\",\n    folder=\"EarthEngine\",  # Change to your Google Drive folder\n    fileNamePrefix=\"gedi_mosaic\",  # No \".tif\" extension\n    scale=gridScale,\n    region=geometry,\n    maxPixels=1e10,\n)\n\n\n\n\nCode\npredictors = embeddingsImage.bandNames()\n\n\n\n\nCode\npredicted = gediMosaic.bandNames().get(0)\n\n\n\n\nCode\nprint(\"predictors\", predictors.getInfo())\n\n\n\n\nCode\nprint(\"predicted\", predicted.getInfo())\n\n\n\n\nCode\npredictorImage = stackedResampled.select(predictors)\n\n\n\n\nCode\npredictedImage = stackedResampled.select([predicted])\n\n\n\n\nCode\nclassMask = predictedImage.mask().toInt().rename(\"class\")\n\n\n\n\nCode\nnumSamples = 1000\n\n\n\n\nCode\ntraining = stackedResampled.addBands(classMask).stratifiedSample(\n    numPoints=numSamples,\n    classBand=\"class\",\n    region=geometry,\n    scale=gridScale,\n    classValues=[0, 1],\n    classPoints=[0, numSamples],\n    dropNulls=True,\n    tileScale=16,\n)\n\n\n\n\nCode\nmodel = (\n    ee.Classifier.smileRandomForest(50)\n    .setOutputMode(\"REGRESSION\")\n    .train(features=training, classProperty=predicted, inputProperties=predictors)\n)\n\n\n\n\nCode\npredicted = training.classify(classifier=model, outputName=\"agbd_predicted\")\n\n\n\n\nCode\ndef calculatermse(inputs):\n    observed = ee.Array(inputs.aggregate_array(\"agbd\"))\n    predicted = ee.Array(inputs.aggregate_array(\"agbd_predicted\"))\n    rmse = (\n        observed.subtract(predicted)\n        .pow(2)\n        .reduce(ee.Reducer.mean(), [0])\n        .sqrt()\n        .get([0])\n    )\n    return rmse\n\n\n\n\nCode\nrmse = calculatermse(predicted)\nprint(\"RMSE\", rmse.getInfo())"
  },
  {
    "objectID": "satellite-embedding/regression.html#plot",
    "href": "satellite-embedding/regression.html#plot",
    "title": "Visulization",
    "section": "Plot",
    "text": "Plot\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt"
  }
]